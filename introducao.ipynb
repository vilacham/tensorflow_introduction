{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao TensorFlow\n",
    "\n",
    "\n",
    "<img src=\"img/tensorflow.png\" width=150px>\n",
    "\n",
    "Ao longo desta aula, você aplicará seus conhecimentos de redes neurais em conjuntos de dados reais usando o [TensorFlow](https://www.tensorflow.org), uma biblioteca de código aberto de Deep Learning criada pela Google. Você usará o TensorFlow para classificar imagens do conjunto de dados notMNIST - um conjunto de imagens em inglês de A até J. Você pode ver alguns exemplos abaixo.\n",
    "\n",
    "<img src=\"img/notmnist.png\" width=400px>\n",
    "\n",
    "Seu objetivo será detectar automaticamente a letra baseada na imagem do conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Instalação\n",
    "\n",
    "Um arquivo de ambiente com todos os pacotes necessários para o acompanhamento da aula foi criado e está no mesmo repositório que este notebook (procure por `tf_intro.yaml`). Use o comando abaixo para criar o ambiente a partir dele:\n",
    "\n",
    "`conda env create -f tf_intro.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após criar e entrar no ambiente `tf_intro`, execute a célula abaixo para garantir que tudo está instalado corretamente. O output deve ser \"Ola, mundo!\". Não se preocupe em entender o que está acontecendo, explicações serão dadas ao longo do notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Ola, mundo!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello_constant = tf.constant('Ola, mundo!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \"Olá, mundo\" do TensorFlow\n",
    "\n",
    "Nesta seção, vamos analisar o script que foi executado na célula acima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor\n",
    "\n",
    "No TensorFlow, os objetos não são salvos em integers, floats ou strings. estes valores são encapsulados em um objeto chamado tensor. No caso de `hello_constant = tf.constant('Ola, mundo!')`, `hello_constant` é um tensor string de 0 dimensões, mas os tensores podem ter uma variedade de tamanhos, como exposto abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A é um tensor int32 de 0 dimensões\n",
    "A = tf.constant(1234)\n",
    "\n",
    "# B é um tensor int32 de 1 dimensão\n",
    "B = tf.constant([123, 456, 789])\n",
    "\n",
    "# C é um tensor int32 de 2 dimensões\n",
    "C = tf.constant([[123, 456, 789], [222, 333, 444]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**`tf.constant`**](https://www.tensorflow.org/api_docs/python/tf/constant) é uma das diversas operações do TensorFlow que usaremos neste notebook. O tensor retornado por [**`tf.constant`**](https://www.tensorflow.org/api_docs/python/tf/constant) é o que chamamos de tensor constante, pois seu valor nunca muda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessão\n",
    "\n",
    "A API do TensorFlow é construída em volta da ideia de um grafo computacional, um modo de visualizar processos matemáticos que é discutido no repositório do MiniFlow **(incluir link para repo)**. A figura abaixo ilustra o código do \"Olá, mundo\" transformado em um grafo:\n",
    "\n",
    "<img src=\"img/session.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma \"Sessão do TensorFlow\", como mostrada acima, é um abiente para rodar um grafo. A sessão é responsável por alocar as operações para as GPU(s) e/ou CPU(s), incluindo máquinas remotas. Vamos executar o \"Olá, mundo\" mais uma vez e logo depois entender como funciona a sessão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Ola, mundo!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello_constant = tf.constant('Ola, mundo!')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código cria o tensor `hello_constant` nas linhas iniciais. O próximo passo é avaliar o tensor na sessão.\n",
    "\n",
    "O código cria uma instância de sessão, `sess`, usando [**`tf.Session`**](https://www.tensorflow.org/api_docs/python/tf/Session). A função [**`sess.run()`**](https://www.tensorflow.org/api_docs/python/tf/Session#run) então avalia o tensor e retorna os resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Input\n",
    "\n",
    "Na seção anterior, passamos um tensor por uma sessão e ele retornou um resultado. E se quisermos usar algo não constante? Aqui é onde a função [**`tf.placeholder()`**](https://www.tensorflow.org/api_docs/python/tf/placeholder) e o `feed_dict` aparecem. Nesta seção, apresentaremos o básico sobre como introduzir dados no TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.placeholder()\n",
    "\n",
    "Infelizmente, não é possível alocar `x` para o conjunto de dados e colocar isso no TensorFlow, porque com o tempo você irá querer que o seu modelo receba diferentes conjuntos de dados com diferentes parâmetros. Para isso, você precisará do [**`tf.placeholder()`**](https://www.tensorflow.org/api_docs/python/tf/placeholder)!\n",
    "\n",
    "[**`tf.placeholder()`**](https://www.tensorflow.org/api_docs/python/tf/placeholder) retorna um tensor que pega o valor dos dados passados para a função [**`tf.session.run()`**](https://www.tensorflow.org/api_docs/python/tf/Session#run), permitindo que você decida o input logo antes da sessão rodar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feed_dict\n",
    "\n",
    "O parâmetro `feed_dict` é usado na [**`tf.session.run()`**](https://www.tensorflow.org/api_docs/python/tf/Session#run) para alocar o tensor placeholder. O exemplo abaixo mostra o tensor `x` recebendo a string `Ola, mundo`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(x, feed_dict={x: 'Ola, mundo'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possível alocar mais de um tensor usando o `feed_dict`, como feito abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.string)\n",
    "y = tf.placeholder(tf.int32)\n",
    "z = tf.placeholder(tf.float32)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    outpupt = sess.run(x, feed_dict={x: 'Teste', y: 123, z: 45.67})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso os dados passados para o `feed_dict` não combinem com o tipo do tensor e não possam ser lançados no tipo do tensor, você obterá o erro \"`ValueError: invalid literal for`...\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Matemática com TensorFlow\n",
    "\n",
    "Conseguir o input é ótimo, mas agora precisamos usá-lo. Nesta seção, usaremos as funções matemáticas mais conhecidas (adição, subtração, multiplicação e divisão) com tensores. Existem muitas outras funções matemáticas que podem ser encontradas na [documentação](https://www.tensorflow.org/api_docs/python/math_ops/) do Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adição\n",
    "\n",
    "Começaremos com a função de adição. A função [**`tf.add()`**](https://www.tensorflow.org/api_docs/python/tf/add) faz exatamente o que esperamos que ela faça: recebe dois números, dois tensores ou um de cada, e retorna a soma deles como um tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(5, 2)    # Retorna 7\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subratação, multiplicação e divisão\n",
    "\n",
    "Assim como a função de adição, as funções de subratação, multiplicação e divisão são bastante intuitivas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "75\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "a = tf.subtract(15, 5)    # Retorna 10\n",
    "b = tf.multiply(15, 5)    # Retorna 75\n",
    "c = tf.div(15, 5)         # Retorna 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(b))\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertendo tipos\n",
    "\n",
    "Pode ser necessário converter tipos para fazer alguns operadores trabalharem juntos. Por exemplo, o código `tf.subtract(tf.constant(2.0), tf.constant(1))` falha ao ser executad.\n",
    "\n",
    "Isso acontece pois `1` é um interger e a constante `2.0` é um ponto flutuante. A operação `subtract` espera que eles combinem.\n",
    "\n",
    "Em casos como este, você pode tanto garantir que os dados sejam sempre do mesmo tipo quanto mudar o tipo de um determinado valor. Neste caso, converter `2.0` para um interger antes de subtrair irá gerar o resultado certo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo\n",
    "\n",
    "Na célula a seguir, usaremos o TensorFlow para imprimir o resultado da expressão numérica `10 / 2 - 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.constant(10)\n",
    "y = tf.constant(2)\n",
    "result = tf.subtract(tf.div(x, y), 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Função linear no TensorFlow\n",
    "\n",
    "A operação ais comum nas redes neurais é calcular a combinação linear de inputs, pesos e vieses. Para lembrar, nós podemos escrever o output da operação linear como\n",
    "\n",
    "$$\n",
    "y = xW + b\n",
    "$$\n",
    "\n",
    "Aqui, **W** é a matriz dos pesos conectando duas camadas. O output **y**, o input **x** e os vieses **b** são todos vetores.\n",
    "\n",
    "O objetivo de treinar uma rede neural é modificar os pesos e vieses para prever os rótulos de modo mais eficiente. A fim de usar pesos e viés, você precisará de um tensor que pode ser modificado. Isso elimina o [**`tf.placeholder()`**](https://www.tensorflow.org/api_docs/python/tf/placeholder) e o [**`tf.constant()`**](https://www.tensorflow.org/api_docs/python/tf/constant), uma vez que esses tensores não podem ser modificados. Aqui é onde aparece a classe [**`tf.Variable`**](https://www.tensorflow.org/api_docs/python/tf/Variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.Variable()\n",
    "\n",
    "A classe [**`tf.Variable`**](https://www.tensorflow.org/api_docs/python/tf/Variable) cria um tensor com um valor inicial que pode ser modificado, tal como uma variável comum do Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse tensor guarda o estado em ua sessão, então é necessário inicializar o estado do tensor manualmente. Usaremos a função [**`tf.global_variables_initializer()`**](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer) para inicializar o estado de todos os tensores Variáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao chamar [**`tf.global_variables_initializer()`**](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer), a função retorna uma operação que irá inicializar todas as variáveis TensorFlow do grafo. Chama-se a operação usando uma sessão para inicializar todas as variáveis, como mostrado acima. Usar a classe [**`tf.Variable`**](https://www.tensorflow.org/api_docs/python/tf/Variable) permite que mudemos os pesos e o viés, mas os alores iniciais precisam ser escolhidos.\n",
    "\n",
    "Inicializar os pesos com números aleatórios a partir de uma distribuição normal é uma boa prática. Randomizar os pesos ajuda o modelo a não ficar preso sempre no mesmo lugar toda vez que for treinado.\n",
    "\n",
    "De modo similar, escolher os pesos de uma distribuição normal previne que um peso se sobreponha aos outros. Usaremos a função [**`tf.truncated_normal()`**](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) para gerar números aleatórios a partir de uma distribuição normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.truncated_normal()\n",
    "\n",
    "A função [**`tf.truncated_normal()`**](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) retorna um tensor com valores aleatórios vindos de uma distribuição normal cuja magnitude é de não mais que 2 desvios padrão da média."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 120\n",
    "n_labels = 5\n",
    "weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que os pesos já auxiliam o modelo a não se prender, não é necessário randomizar também o viés. Vamos usar a solução mais simples que é definir o viés como 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.zeros()\n",
    "\n",
    "A função [**`tf.zeros()`**](https://www.tensorflow.org/api_docs/python/tf/zeros) retorna um tensor composto de zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 5\n",
    "bias = tf.Variable(tf.zeros(n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Neste exercício, você deve classificar números `0`, `1` e `2` escritos a mão do conjunto de dados MNIST usando o TensorFlow. A figura abaixo mostra uma pequena amostra dos dados que serão usados para o treinamento. Repare que alguns dos `1` foram escritos com uma serifa no topo e com ângulos diferentes. As similaridades e diferenças terão um papel importante na definição dos pesos do modelo.\n",
    "\n",
    "<img src='img/mnist_012.png'>\n",
    "\n",
    "As imagens abaixo são os pesos treinados para cada rótulo (`0`, `1` e `2`, da esquerda para a direita). Os pesos mostram as propriedades únicas de cada dígito que eles encontraram.\n",
    "\n",
    "<img src='img/weights_012.png' width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siga as instruções abaixo para treinar seus próprios pesos:\n",
    "1. Abra o arquivo funcao_linear.py\n",
    "     1. Implemente `gerar_pesos` de modo que retorne um `tf.Variable` com pesos aleatórios\n",
    "     2. Implemente `gerar_vies` de modo que retorne um `tf.Variable` preenchido com zeros.\n",
    "     3. Implemente `linear`.\n",
    "2. Inicialize todos os pesos antes de executar a célula abaixo.\n",
    "\n",
    "**Nota**: uma vez que $xW$ em $xW + b$ é uma multiplicação de matrizes, é necessário usar a função [**`tf.matmul()`**](https://www.tensorflow.org/api_docs/python/tf/matmul) ao invés de [**`tf.multiply()`**](https://www.tensorflow.org/api_docs/python/tf/multiply). Não se esqueça que a ordem é importante na multiplicação de matrizes, então `tf.matmul(a, b)` não é a mesma coisa que `tf.matmul(b, a)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tmp/datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting tmp/datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Loss: 5.477291107177734\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from funcao_linear import gerar_pesos, gerar_vies, linear\n",
    "from auxiliar import mnist_features_labels\n",
    "\n",
    "# Definição do número de atributos e de rótulos\n",
    "n_features = 28 * 28    # Imagens do MNIST têm dimensões 28x28\n",
    "n_labels = 3            # Apenas os três primeiros rótulos (0, 1 e 2)\n",
    "\n",
    "# Atributos e rótulos\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# TODO: Chamar as funções implementadas em funcao_linear.py\n",
    "w = gerar_pesos(n_features, n_labels)\n",
    "b = gerar_vies(n_labels)\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Obter dados de treinamento\n",
    "X, y = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # TODO: Inicialize as variáveis na sessão\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Entropia cruzada\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), \n",
    "                                   reduction_indices=1)\n",
    "    \n",
    "    # Perda\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # Taxa de aprendizado\n",
    "    learning_rate = 0.08\n",
    "    \n",
    "    # Gradiente descendente\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    # Executar otimizador e obter perda\n",
    "    _, l = sess.run([optimizer, loss], feed_dict={features: X, labels:y})\n",
    "\n",
    "# Imprimir perda\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Softmax no TensorFlow\n",
    "\n",
    "A função softmax comprime os inputs, normalmente chamados de **logits**, que são números entre 0 e 1 e também normaliza os outputs de modo que todos eles somados resultem em 1. Isso significa que o output da função softmax é equivalente Pa distribuição de probabilidade entre categorias. É a função perfeita para ser usada como ativação de output em uma rede prevendo múltiplas classes.\n",
    "\n",
    "<img src='img/softmax_input_output.png' width=500px>\n",
    "\n",
    "Estamos usando o TensorFlow para construir redes neurais e, convenientemente, existe uma função para calcular o softmax. A função [**`tf.nn.softmax()`**](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) recebe os logits e retorna as ativações softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6590012  0.24243298 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "logit_data = [2.0, 1.0, 0.1]\n",
    "\n",
    "logits = tf.placeholder(tf.float32)\n",
    "softmax = tf.nn.softmax(logits)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Entropia cruzada no TensorFlow\n",
    "\n",
    "Como visto anteriormente no curso, a entropia cruzada pode ser usada como função de custo em classificações codificadas one-hot. \n",
    "\n",
    "<img src='img/cross_entropy_diagram.png' width=500px>\n",
    "\n",
    "A partir da figura acima, conclue-se que é possível criar uma função de entropia cruzada no TensorFlow. Para tanto, são necessárias duas funções: [**`tf.reduce_sum()`**](https://www.tensorflow.org/api_docs/python/tf/reduce_sum) e [**`tf.log()`**](https://www.tensorflow.org/api_docs/python/tf/log)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reduce_sum()\n",
    "\n",
    "A função [**`tf.reduce_sum()`**](https://www.tensorflow.org/api_docs/python/tf/reduce_sum) soma todos os números de uma array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.reduce_sum([1, 2, 3, 4, 5])    # Retorna 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.log()\n",
    "\n",
    "A função [**`tf.log()`**](https://www.tensorflow.org/api_docs/python/tf/log) retorna o log natural de um número."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.log(100.0)    # Retorna 4.60517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementação\n",
    "\n",
    "Na célula abaixo, vamos implementar a função de entropia cruzada usando TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667497\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "softmax_data = [0.7, 0.2, 0.1]\n",
    "one_hot_data = [1.0, 0.0, 0.0]\n",
    "\n",
    "softmax = tf.placeholder(tf.float32)\n",
    "one_hot = tf.placeholder(tf.float32)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(tf.multiply(one_hot, tf.log(softmax)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(cross_entropy, feed_dict={softmax: softmax_data, \n",
    "                                             one_hot: one_hot_data}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Minibatch no TensorFlow\n",
    "\n",
    "Minibatch (miniloteamento em português) é a técnica utilizada para treinar em subconjuntos do conjunto de dados ao invés de usar todos os dados de uma vez. Isso permite treinar um modelo mesmo se o computador não tiver a memória para guardar todo o conjunto de dados.\n",
    "\n",
    "O minibatch é computacionalmente ineficiente, uma vez que você não pode calcular a perda simultaneamente através de todas as amostras. No entanto, esse é um pequeno preço a se pagar para poder pelo menos rodar um modelo.\n",
    "\n",
    "Essa técnica também é útil ao ser combinada com a SGD. A ideia é embaralhar aleatoriamente os dados no começo de cada época, então criar os minilotes. Para cada minilote, treina-se os pesos da rede com gradiente descendente. Uma vez que os lotes são aleatórios, está sendo realizado uma SGD com cada lote.\n",
    "\n",
    "Vamos olhar o conjunto de dados do MNIST com pesos e viés para ver se sua máquina consegue lidar com ele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "n_input = 28 * 28    # MNIST data input (image shape: 28x28)\n",
    "n_classes = 10       # MNIST total classes (0-9 digits)\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/datasets/ud730/mnist', \n",
    "                                  one_hot=True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nas células a seguir, vamos calcular quantos bytes de memória as variáveis `train_features`, `train_labels`, `weights` e `bias` ocupam. Caso você não saiba quanta memória um float32 exige, recomendamos a leitura [deste link](https://en.wikipedia.org/wiki/Single-precision_floating-point_format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(784, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "variables = [train_features, train_labels, weights, bias]\n",
    "\n",
    "for variable in variables:\n",
    "    print(variable.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features: 172480000 bytes\n",
      "train_features: 2200000 bytes\n",
      "train_features: 31360 bytes\n",
      "train_features: 40 bytes\n",
      "Total: 174711400 bytes\n"
     ]
    }
   ],
   "source": [
    "train_features_size = 4 * variables[0].shape[0] * variables[0].shape[1]\n",
    "print('train_features: {} bytes'.format(train_features_size))\n",
    "\n",
    "train_labels_size = 4 * variables[1].shape[0] * variables[1].shape[1]\n",
    "print('train_features: {} bytes'.format(train_labels_size))\n",
    "\n",
    "weights_size = 4 * variables[2].shape[0] * variables[2].shape[1]\n",
    "print('train_features: {} bytes'.format(weights_size))\n",
    "\n",
    "bias_size = 4 * variables[3].shape[0]\n",
    "print('train_features: {} bytes'.format(bias_size))\n",
    "\n",
    "print('Total: {} bytes'.format(train_features_size + train_labels_size \n",
    "                               + weights_size + bias_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O total de espaço na memória necessário para os inputs, pesos e viés é de aproximadamente 174 MB, o que não é muito. É possível treinar esse conjunto de dados na maioria dos CPUs e GPUs. Mas conjuntos de dados maiores que aparecerão no futuro serão medidos em GB ou até mais. É possível comprar mais memória, mas isso é caro. Uma placa de vídeo (GPU) Titan X com 12 GB de memória custa mais de R$ 4.000,00. Ao invés disso, para rodar modelos grandes na sua máquina, utiliza-se o miniloteamento.\n",
    "\n",
    "Infelizmente, muitas vezes não é possível dividir os dados em lotes de tamanhos exatamente iguais. Por exemplo, imagine que você quer criar lotes de tamanho 128 cada um com um conjunto de dados com 1000 amostras. Uma vez que 128 não divide 1000 sem deixar resto, você terminará o processo com 7 lotes de 128 amostras e 1 lote com 104 amostrar (7 * 128 + 1 * 104 = 1000).\n",
    "\n",
    "Neste caso, o tamanho das amostras varia, então é importante se aproveitar da função do TensorFlow [**`tf.placeholder()`**](https://www.tensorflow.org/api_docs/python/tf/placeholder) para receber os tamanhos variáveis dos lotes.\n",
    "\n",
    "Continuando com o exemplo do MNIST, se cada amostra tiver 784 características (`n_input = 28 * 28`) e 10 possíveis rótulos (`n_classes = 10`), as dimensões de `features` seriam `[None, n_input]` e de `labels` seriam `[None, n_classes]`. \n",
    "\n",
    "A dimensão `None` é um marcador de posição para o tamanho do lote. Na hora de executar, o TensorFlow aceita qualquer tamanho de lote maior que 0. A configuração abaixo permite que você forneça `features` e `labels` para o modelo seja com lotes de 128 amostras, seja com lote lotes de 104 amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fixar toda a ideia, vamos criar um exemplo na célula abaixo e calcular quantos lotes teremos e qual será o tamanho do último lote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de lotes: 391\n",
      "Tamanho do ultimo lote: 80\n"
     ]
    }
   ],
   "source": [
    "# features.shape = (50000, 400)\n",
    "# labels.shape = (50000, 10)\n",
    "# batch_size = 128\n",
    "\n",
    "print('Quantidade de lotes: {}'.format(50000 // 128 + 1))\n",
    "print('Tamanho do ultimo lote: {}'.format(50000 % 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício\n",
    "\n",
    "Neste exercício, você deve implementar a função `batches` no arquivo mini_batches.py e testá-lo na célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[['F11', 'F12', 'F13', 'F14'],\n",
      "   ['F21', 'F22', 'F23', 'F24'],\n",
      "   ['F31', 'F32', 'F33', 'F34']],\n",
      "  [['L11', 'L12'], ['L21', 'L22'], ['L31', 'L32']]],\n",
      " [[['F41', 'F42', 'F43', 'F44']], [['L41', 'L42']]]]\n"
     ]
    }
   ],
   "source": [
    "from mini_batches import batches\n",
    "from pprint import pprint\n",
    "\n",
    "# 4 samples of features\n",
    "example_features = [['F11', 'F12', 'F13', 'F14'],\n",
    "                    ['F21', 'F22', 'F23', 'F24'],\n",
    "                    ['F31', 'F32', 'F33', 'F34'],\n",
    "                    ['F41', 'F42', 'F43', 'F44']]\n",
    "\n",
    "# 4 samples of labels\n",
    "example_labels = [['L11', 'L12'],\n",
    "                  ['L21', 'L22'],\n",
    "                  ['L31', 'L32'],\n",
    "                  ['L41', 'L42']]\n",
    "\n",
    "# pprint imprime estruturas de dados como arrays 2D para tornar a leitura\n",
    "# mais fácil\n",
    "pprint(batches(3, example_features, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que você se certificou que a função `batches` foi implementada corretamente, utilize-a para alimentar um modelo linear com rótulos e atributos do MNIST. Para isso, você deve configurar o tamanho do lote e rodar o otimizador por todos os lotes com a função implementada. O tamanho do lote recomendado é 128 (caso tenha restrições de memória, sinta-se livre para mudar o tamanho dos lotes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Extracting tmp/datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting tmp/datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting tmp/datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting tmp/datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "Test accuracy: 0.09470000118017197\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from funcao_linear import linear\n",
    "from mini_batches import batches\n",
    "\n",
    "n_features = 28 * 28\n",
    "n_labels = 3\n",
    "learning_rate = 0.001\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/datasets/ud730/mnist', \n",
    "                                  one_hot=True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "X, y = mnist_features_labels(n_labels)\n",
    "\n",
    "# TODO: Configure o tamanho do mini-lote\n",
    "batch_size = 128\n",
    "assert batch_size is not None, 'Configure batch_size'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # TODO: Treinar o otimizador em todos os mini-lotes\n",
    "    batches = batches(batch_size, train_features, train_labels)\n",
    "    for batch_features, batch_labels in batches:\n",
    "        sess.run(optimizer, feed_dict={features: batch_features,\n",
    "                                       labels: batch_labels})\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features,\n",
    "                                                  labels: test_labels})\n",
    "\n",
    "# Imprimir perda\n",
    "print('Test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A acurácia é baixa, mas você já sabe que pode treinar no conjunto de dados mais de uma vez. É possível treinar um modelo usando o conjunto de dados múltiplas vezes. Trataremos deste assunto na próxima sessão, onde falaremos sobre épocas (epochs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Épocas no TensorFlow\n",
    "\n",
    "Uma época é uma única passagem para frente e para trás do conjunto completo de dados. Isso serve para aumentar a precisão do modelo sem exigir mais dados. Nesta seção, mostraremos como implementar a ideia de épocas usando TensorFlow e também como escolher o melhor número de épocas.\n",
    "\n",
    "O código da célula seguinte importa as imagens do MNIST e realiza todas as operações necessárias antes de se iniciar uma sessão que não dependem dos hiperparâmetros `batch_size`, `epochs` e `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from mini_batches import batches\n",
    "\n",
    "n_input = 28 * 28\n",
    "n_classes = 10\n",
    "\n",
    "mnist = input_data.read_data_sets('/tmp/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "train_features = mnist.train.images\n",
    "validation_features = mnist.validation.images\n",
    "test_features = mnist.test.images\n",
    "\n",
    "train_labels = mnist.train.labels.astype(np.float32)\n",
    "validation_labels = mnist.validation.labels.astype(np.float32)\n",
    "test_labels = mnist.test.labels.astype(np.float32)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir cria uma função que imprime a perda e a acurácia de uma época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_epoch_stats(epoch, session, last_features, last_labels):\n",
    "    \"\"\" Imprime informações de uma época do treinamento\n",
    "    \n",
    "    Parâmetros\n",
    "    ----------\n",
    "    epoch: Época do treinamento\n",
    "    session: Sessão do TensorFlow\n",
    "    last_features: Atributos usados no treinamento\n",
    "    last_labels: Rótulos usados no treinamento\n",
    "    \"\"\"\n",
    "    current_loss = sess.run(loss, feed_dict={features: last_features,\n",
    "                                             labels: last_labels})\n",
    "    validation_accuracy = sess.run(accuracy, feed_dict={features: validation_features,\n",
    "                                                        labels: validation_labels})\n",
    "    print('Epoch: {:4d} | Loss: {:7.4f} | Accuracy: {:.4f}'.format(epoch + 1, current_loss, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima célula, vamos treinar um modelo usando 10 épocas, com uma taxa de aprendizado de 0.001 e um mini-lote com tamanho 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 | Loss: 11.8811 | Accuracy: 0.0770\n",
      "Epoch:    2 | Loss: 11.0033 | Accuracy: 0.0848\n",
      "Epoch:    3 | Loss: 10.3354 | Accuracy: 0.0956\n",
      "Epoch:    4 | Loss:  9.8120 | Accuracy: 0.1040\n",
      "Epoch:    5 | Loss:  9.3684 | Accuracy: 0.1142\n",
      "Epoch:    6 | Loss:  8.9691 | Accuracy: 0.1260\n",
      "Epoch:    7 | Loss:  8.6014 | Accuracy: 0.1406\n",
      "Epoch:    8 | Loss:  8.2612 | Accuracy: 0.1576\n",
      "Epoch:    9 | Loss:  7.9459 | Accuracy: 0.1754\n",
      "Epoch:   10 | Loss:  7.6528 | Accuracy: 0.1942\n",
      "Test accuracy: 0.2018\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_features, batch_labels in batches:\n",
    "            sess.run(optimizer, feed_dict={features: batch_features,\n",
    "                                           labels: batch_labels,\n",
    "                                           learning_rate: lr})\n",
    "        print_epoch_stats(epoch, sess, batch_features, batch_labels)\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features,\n",
    "                                                  labels: test_labels})\n",
    "\n",
    "print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que cada época tenta minimizar a perda, resultando em uma acurácia maior. O modelo acima continua a melhorar a acurácia até a última época. Pensando nisso, vamos aumentar a quantidade de épocas para 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 | Loss: 10.4622 | Accuracy: 0.1308\n",
      "Epoch:    2 | Loss:  9.4286 | Accuracy: 0.1374\n",
      "Epoch:    3 | Loss:  8.6872 | Accuracy: 0.1464\n",
      "Epoch:    4 | Loss:  8.1149 | Accuracy: 0.1548\n",
      "Epoch:    5 | Loss:  7.6576 | Accuracy: 0.1662\n",
      "Epoch:    6 | Loss:  7.2732 | Accuracy: 0.1832\n",
      "Epoch:    7 | Loss:  6.9366 | Accuracy: 0.2016\n",
      "Epoch:    8 | Loss:  6.6351 | Accuracy: 0.2244\n",
      "Epoch:    9 | Loss:  6.3604 | Accuracy: 0.2422\n",
      "Epoch:   10 | Loss:  6.1079 | Accuracy: 0.2636\n",
      "Epoch:   11 | Loss:  5.8743 | Accuracy: 0.2802\n",
      "Epoch:   12 | Loss:  5.6576 | Accuracy: 0.2992\n",
      "Epoch:   13 | Loss:  5.4564 | Accuracy: 0.3162\n",
      "Epoch:   14 | Loss:  5.2695 | Accuracy: 0.3366\n",
      "Epoch:   15 | Loss:  5.0954 | Accuracy: 0.3484\n",
      "Epoch:   16 | Loss:  4.9331 | Accuracy: 0.3686\n",
      "Epoch:   17 | Loss:  4.7817 | Accuracy: 0.3842\n",
      "Epoch:   18 | Loss:  4.6401 | Accuracy: 0.3990\n",
      "Epoch:   19 | Loss:  4.5077 | Accuracy: 0.4092\n",
      "Epoch:   20 | Loss:  4.3838 | Accuracy: 0.4248\n",
      "Epoch:   21 | Loss:  4.2679 | Accuracy: 0.4342\n",
      "Epoch:   22 | Loss:  4.1592 | Accuracy: 0.4456\n",
      "Epoch:   23 | Loss:  4.0573 | Accuracy: 0.4562\n",
      "Epoch:   24 | Loss:  3.9616 | Accuracy: 0.4670\n",
      "Epoch:   25 | Loss:  3.8716 | Accuracy: 0.4770\n",
      "Epoch:   26 | Loss:  3.7867 | Accuracy: 0.4866\n",
      "Epoch:   27 | Loss:  3.7067 | Accuracy: 0.4942\n",
      "Epoch:   28 | Loss:  3.6311 | Accuracy: 0.5020\n",
      "Epoch:   29 | Loss:  3.5595 | Accuracy: 0.5090\n",
      "Epoch:   30 | Loss:  3.4917 | Accuracy: 0.5158\n",
      "Epoch:   31 | Loss:  3.4273 | Accuracy: 0.5234\n",
      "Epoch:   32 | Loss:  3.3661 | Accuracy: 0.5322\n",
      "Epoch:   33 | Loss:  3.3079 | Accuracy: 0.5406\n",
      "Epoch:   34 | Loss:  3.2524 | Accuracy: 0.5472\n",
      "Epoch:   35 | Loss:  3.1995 | Accuracy: 0.5566\n",
      "Epoch:   36 | Loss:  3.1490 | Accuracy: 0.5646\n",
      "Epoch:   37 | Loss:  3.1007 | Accuracy: 0.5714\n",
      "Epoch:   38 | Loss:  3.0545 | Accuracy: 0.5764\n",
      "Epoch:   39 | Loss:  3.0102 | Accuracy: 0.5804\n",
      "Epoch:   40 | Loss:  2.9678 | Accuracy: 0.5860\n",
      "Epoch:   41 | Loss:  2.9271 | Accuracy: 0.5892\n",
      "Epoch:   42 | Loss:  2.8881 | Accuracy: 0.5950\n",
      "Epoch:   43 | Loss:  2.8506 | Accuracy: 0.5982\n",
      "Epoch:   44 | Loss:  2.8145 | Accuracy: 0.6046\n",
      "Epoch:   45 | Loss:  2.7798 | Accuracy: 0.6086\n",
      "Epoch:   46 | Loss:  2.7464 | Accuracy: 0.6114\n",
      "Epoch:   47 | Loss:  2.7142 | Accuracy: 0.6148\n",
      "Epoch:   48 | Loss:  2.6832 | Accuracy: 0.6204\n",
      "Epoch:   49 | Loss:  2.6532 | Accuracy: 0.6248\n",
      "Epoch:   50 | Loss:  2.6243 | Accuracy: 0.6284\n",
      "Epoch:   51 | Loss:  2.5963 | Accuracy: 0.6322\n",
      "Epoch:   52 | Loss:  2.5693 | Accuracy: 0.6348\n",
      "Epoch:   53 | Loss:  2.5431 | Accuracy: 0.6384\n",
      "Epoch:   54 | Loss:  2.5177 | Accuracy: 0.6420\n",
      "Epoch:   55 | Loss:  2.4931 | Accuracy: 0.6460\n",
      "Epoch:   56 | Loss:  2.4693 | Accuracy: 0.6488\n",
      "Epoch:   57 | Loss:  2.4462 | Accuracy: 0.6518\n",
      "Epoch:   58 | Loss:  2.4237 | Accuracy: 0.6554\n",
      "Epoch:   59 | Loss:  2.4019 | Accuracy: 0.6578\n",
      "Epoch:   60 | Loss:  2.3806 | Accuracy: 0.6604\n",
      "Epoch:   61 | Loss:  2.3600 | Accuracy: 0.6628\n",
      "Epoch:   62 | Loss:  2.3399 | Accuracy: 0.6650\n",
      "Epoch:   63 | Loss:  2.3204 | Accuracy: 0.6666\n",
      "Epoch:   64 | Loss:  2.3013 | Accuracy: 0.6688\n",
      "Epoch:   65 | Loss:  2.2828 | Accuracy: 0.6706\n",
      "Epoch:   66 | Loss:  2.2647 | Accuracy: 0.6728\n",
      "Epoch:   67 | Loss:  2.2471 | Accuracy: 0.6760\n",
      "Epoch:   68 | Loss:  2.2299 | Accuracy: 0.6784\n",
      "Epoch:   69 | Loss:  2.2132 | Accuracy: 0.6816\n",
      "Epoch:   70 | Loss:  2.1968 | Accuracy: 0.6830\n",
      "Epoch:   71 | Loss:  2.1808 | Accuracy: 0.6844\n",
      "Epoch:   72 | Loss:  2.1653 | Accuracy: 0.6868\n",
      "Epoch:   73 | Loss:  2.1500 | Accuracy: 0.6884\n",
      "Epoch:   74 | Loss:  2.1352 | Accuracy: 0.6900\n",
      "Epoch:   75 | Loss:  2.1206 | Accuracy: 0.6918\n",
      "Epoch:   76 | Loss:  2.1064 | Accuracy: 0.6926\n",
      "Epoch:   77 | Loss:  2.0925 | Accuracy: 0.6954\n",
      "Epoch:   78 | Loss:  2.0790 | Accuracy: 0.6980\n",
      "Epoch:   79 | Loss:  2.0657 | Accuracy: 0.6998\n",
      "Epoch:   80 | Loss:  2.0527 | Accuracy: 0.7010\n",
      "Epoch:   81 | Loss:  2.0400 | Accuracy: 0.7024\n",
      "Epoch:   82 | Loss:  2.0275 | Accuracy: 0.7044\n",
      "Epoch:   83 | Loss:  2.0153 | Accuracy: 0.7066\n",
      "Epoch:   84 | Loss:  2.0034 | Accuracy: 0.7082\n",
      "Epoch:   85 | Loss:  1.9917 | Accuracy: 0.7096\n",
      "Epoch:   86 | Loss:  1.9803 | Accuracy: 0.7102\n",
      "Epoch:   87 | Loss:  1.9690 | Accuracy: 0.7126\n",
      "Epoch:   88 | Loss:  1.9580 | Accuracy: 0.7146\n",
      "Epoch:   89 | Loss:  1.9472 | Accuracy: 0.7160\n",
      "Epoch:   90 | Loss:  1.9366 | Accuracy: 0.7168\n",
      "Epoch:   91 | Loss:  1.9262 | Accuracy: 0.7176\n",
      "Epoch:   92 | Loss:  1.9160 | Accuracy: 0.7190\n",
      "Epoch:   93 | Loss:  1.9060 | Accuracy: 0.7204\n",
      "Epoch:   94 | Loss:  1.8962 | Accuracy: 0.7222\n",
      "Epoch:   95 | Loss:  1.8866 | Accuracy: 0.7242\n",
      "Epoch:   96 | Loss:  1.8771 | Accuracy: 0.7256\n",
      "Epoch:   97 | Loss:  1.8678 | Accuracy: 0.7276\n",
      "Epoch:   98 | Loss:  1.8587 | Accuracy: 0.7300\n",
      "Epoch:   99 | Loss:  1.8497 | Accuracy: 0.7312\n",
      "Epoch:  100 | Loss:  1.8408 | Accuracy: 0.7326\n",
      "Test accuracy: 0.7254\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_features, batch_labels in batches:\n",
    "            sess.run(optimizer, feed_dict={features: batch_features,\n",
    "                                           labels: batch_labels,\n",
    "                                           learning_rate: lr})\n",
    "        print_epoch_stats(epoch, sess, batch_features, batch_labels)\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features,\n",
    "                                                  labels: test_labels})\n",
    "\n",
    "print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível notar que a perda continua a diminuir. Vamos aumentar o número de épocas para 500 e ver o que acontece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 | Loss: 14.3952 | Accuracy: 0.1702\n",
      "Epoch:    2 | Loss: 11.4360 | Accuracy: 0.1908\n",
      "Epoch:    3 | Loss:  9.7691 | Accuracy: 0.1946\n",
      "Epoch:    4 | Loss:  8.8297 | Accuracy: 0.1974\n",
      "Epoch:    5 | Loss:  8.2295 | Accuracy: 0.1976\n",
      "Epoch:    6 | Loss:  7.8052 | Accuracy: 0.2030\n",
      "Epoch:    7 | Loss:  7.4683 | Accuracy: 0.2164\n",
      "Epoch:    8 | Loss:  7.1779 | Accuracy: 0.2268\n",
      "Epoch:    9 | Loss:  6.9164 | Accuracy: 0.2386\n",
      "Epoch:   10 | Loss:  6.6756 | Accuracy: 0.2518\n",
      "Epoch:   11 | Loss:  6.4520 | Accuracy: 0.2636\n",
      "Epoch:   12 | Loss:  6.2436 | Accuracy: 0.2790\n",
      "Epoch:   13 | Loss:  6.0490 | Accuracy: 0.2938\n",
      "Epoch:   14 | Loss:  5.8668 | Accuracy: 0.3072\n",
      "Epoch:   15 | Loss:  5.6956 | Accuracy: 0.3212\n",
      "Epoch:   16 | Loss:  5.5341 | Accuracy: 0.3370\n",
      "Epoch:   17 | Loss:  5.3815 | Accuracy: 0.3522\n",
      "Epoch:   18 | Loss:  5.2371 | Accuracy: 0.3626\n",
      "Epoch:   19 | Loss:  5.1003 | Accuracy: 0.3744\n",
      "Epoch:   20 | Loss:  4.9707 | Accuracy: 0.3860\n",
      "Epoch:   21 | Loss:  4.8478 | Accuracy: 0.3968\n",
      "Epoch:   22 | Loss:  4.7312 | Accuracy: 0.4064\n",
      "Epoch:   23 | Loss:  4.6203 | Accuracy: 0.4142\n",
      "Epoch:   24 | Loss:  4.5149 | Accuracy: 0.4220\n",
      "Epoch:   25 | Loss:  4.4145 | Accuracy: 0.4332\n",
      "Epoch:   26 | Loss:  4.3186 | Accuracy: 0.4434\n",
      "Epoch:   27 | Loss:  4.2270 | Accuracy: 0.4536\n",
      "Epoch:   28 | Loss:  4.1393 | Accuracy: 0.4626\n",
      "Epoch:   29 | Loss:  4.0552 | Accuracy: 0.4712\n",
      "Epoch:   30 | Loss:  3.9744 | Accuracy: 0.4790\n",
      "Epoch:   31 | Loss:  3.8967 | Accuracy: 0.4868\n",
      "Epoch:   32 | Loss:  3.8220 | Accuracy: 0.4946\n",
      "Epoch:   33 | Loss:  3.7499 | Accuracy: 0.5022\n",
      "Epoch:   34 | Loss:  3.6804 | Accuracy: 0.5102\n",
      "Epoch:   35 | Loss:  3.6133 | Accuracy: 0.5166\n",
      "Epoch:   36 | Loss:  3.5484 | Accuracy: 0.5236\n",
      "Epoch:   37 | Loss:  3.4857 | Accuracy: 0.5286\n",
      "Epoch:   38 | Loss:  3.4250 | Accuracy: 0.5384\n",
      "Epoch:   39 | Loss:  3.3663 | Accuracy: 0.5424\n",
      "Epoch:   40 | Loss:  3.3093 | Accuracy: 0.5478\n",
      "Epoch:   41 | Loss:  3.2541 | Accuracy: 0.5544\n",
      "Epoch:   42 | Loss:  3.2006 | Accuracy: 0.5594\n",
      "Epoch:   43 | Loss:  3.1487 | Accuracy: 0.5648\n",
      "Epoch:   44 | Loss:  3.0983 | Accuracy: 0.5688\n",
      "Epoch:   45 | Loss:  3.0493 | Accuracy: 0.5722\n",
      "Epoch:   46 | Loss:  3.0018 | Accuracy: 0.5772\n",
      "Epoch:   47 | Loss:  2.9557 | Accuracy: 0.5826\n",
      "Epoch:   48 | Loss:  2.9109 | Accuracy: 0.5866\n",
      "Epoch:   49 | Loss:  2.8674 | Accuracy: 0.5916\n",
      "Epoch:   50 | Loss:  2.8251 | Accuracy: 0.5962\n",
      "Epoch:   51 | Loss:  2.7841 | Accuracy: 0.6006\n",
      "Epoch:   52 | Loss:  2.7443 | Accuracy: 0.6044\n",
      "Epoch:   53 | Loss:  2.7056 | Accuracy: 0.6076\n",
      "Epoch:   54 | Loss:  2.6680 | Accuracy: 0.6102\n",
      "Epoch:   55 | Loss:  2.6315 | Accuracy: 0.6124\n",
      "Epoch:   56 | Loss:  2.5961 | Accuracy: 0.6152\n",
      "Epoch:   57 | Loss:  2.5617 | Accuracy: 0.6184\n",
      "Epoch:   58 | Loss:  2.5282 | Accuracy: 0.6226\n",
      "Epoch:   59 | Loss:  2.4958 | Accuracy: 0.6260\n",
      "Epoch:   60 | Loss:  2.4643 | Accuracy: 0.6280\n",
      "Epoch:   61 | Loss:  2.4336 | Accuracy: 0.6306\n",
      "Epoch:   62 | Loss:  2.4038 | Accuracy: 0.6338\n",
      "Epoch:   63 | Loss:  2.3749 | Accuracy: 0.6370\n",
      "Epoch:   64 | Loss:  2.3467 | Accuracy: 0.6398\n",
      "Epoch:   65 | Loss:  2.3193 | Accuracy: 0.6422\n",
      "Epoch:   66 | Loss:  2.2926 | Accuracy: 0.6450\n",
      "Epoch:   67 | Loss:  2.2667 | Accuracy: 0.6470\n",
      "Epoch:   68 | Loss:  2.2414 | Accuracy: 0.6510\n",
      "Epoch:   69 | Loss:  2.2168 | Accuracy: 0.6536\n",
      "Epoch:   70 | Loss:  2.1928 | Accuracy: 0.6556\n",
      "Epoch:   71 | Loss:  2.1694 | Accuracy: 0.6582\n",
      "Epoch:   72 | Loss:  2.1466 | Accuracy: 0.6604\n",
      "Epoch:   73 | Loss:  2.1243 | Accuracy: 0.6626\n",
      "Epoch:   74 | Loss:  2.1026 | Accuracy: 0.6656\n",
      "Epoch:   75 | Loss:  2.0814 | Accuracy: 0.6674\n",
      "Epoch:   76 | Loss:  2.0608 | Accuracy: 0.6702\n",
      "Epoch:   77 | Loss:  2.0406 | Accuracy: 0.6732\n",
      "Epoch:   78 | Loss:  2.0209 | Accuracy: 0.6752\n",
      "Epoch:   79 | Loss:  2.0017 | Accuracy: 0.6780\n",
      "Epoch:   80 | Loss:  1.9829 | Accuracy: 0.6804\n",
      "Epoch:   81 | Loss:  1.9646 | Accuracy: 0.6818\n",
      "Epoch:   82 | Loss:  1.9467 | Accuracy: 0.6842\n",
      "Epoch:   83 | Loss:  1.9292 | Accuracy: 0.6856\n",
      "Epoch:   84 | Loss:  1.9121 | Accuracy: 0.6880\n",
      "Epoch:   85 | Loss:  1.8954 | Accuracy: 0.6896\n",
      "Epoch:   86 | Loss:  1.8791 | Accuracy: 0.6912\n",
      "Epoch:   87 | Loss:  1.8631 | Accuracy: 0.6928\n",
      "Epoch:   88 | Loss:  1.8475 | Accuracy: 0.6940\n",
      "Epoch:   89 | Loss:  1.8322 | Accuracy: 0.6956\n",
      "Epoch:   90 | Loss:  1.8173 | Accuracy: 0.6980\n",
      "Epoch:   91 | Loss:  1.8026 | Accuracy: 0.6998\n",
      "Epoch:   92 | Loss:  1.7884 | Accuracy: 0.7010\n",
      "Epoch:   93 | Loss:  1.7744 | Accuracy: 0.7024\n",
      "Epoch:   94 | Loss:  1.7607 | Accuracy: 0.7034\n",
      "Epoch:   95 | Loss:  1.7473 | Accuracy: 0.7044\n",
      "Epoch:   96 | Loss:  1.7342 | Accuracy: 0.7066\n",
      "Epoch:   97 | Loss:  1.7213 | Accuracy: 0.7072\n",
      "Epoch:   98 | Loss:  1.7088 | Accuracy: 0.7082\n",
      "Epoch:   99 | Loss:  1.6965 | Accuracy: 0.7096\n",
      "Epoch:  100 | Loss:  1.6844 | Accuracy: 0.7114\n",
      "Epoch:  101 | Loss:  1.6726 | Accuracy: 0.7130\n",
      "Epoch:  102 | Loss:  1.6611 | Accuracy: 0.7150\n",
      "Epoch:  103 | Loss:  1.6497 | Accuracy: 0.7160\n",
      "Epoch:  104 | Loss:  1.6386 | Accuracy: 0.7180\n",
      "Epoch:  105 | Loss:  1.6277 | Accuracy: 0.7194\n",
      "Epoch:  106 | Loss:  1.6170 | Accuracy: 0.7214\n",
      "Epoch:  107 | Loss:  1.6066 | Accuracy: 0.7226\n",
      "Epoch:  108 | Loss:  1.5963 | Accuracy: 0.7234\n",
      "Epoch:  109 | Loss:  1.5863 | Accuracy: 0.7254\n",
      "Epoch:  110 | Loss:  1.5764 | Accuracy: 0.7270\n",
      "Epoch:  111 | Loss:  1.5667 | Accuracy: 0.7284\n",
      "Epoch:  112 | Loss:  1.5572 | Accuracy: 0.7294\n",
      "Epoch:  113 | Loss:  1.5479 | Accuracy: 0.7310\n",
      "Epoch:  114 | Loss:  1.5387 | Accuracy: 0.7332\n",
      "Epoch:  115 | Loss:  1.5297 | Accuracy: 0.7342\n",
      "Epoch:  116 | Loss:  1.5209 | Accuracy: 0.7354\n",
      "Epoch:  117 | Loss:  1.5122 | Accuracy: 0.7362\n",
      "Epoch:  118 | Loss:  1.5037 | Accuracy: 0.7364\n",
      "Epoch:  119 | Loss:  1.4954 | Accuracy: 0.7368\n",
      "Epoch:  120 | Loss:  1.4872 | Accuracy: 0.7376\n",
      "Epoch:  121 | Loss:  1.4791 | Accuracy: 0.7386\n",
      "Epoch:  122 | Loss:  1.4712 | Accuracy: 0.7388\n",
      "Epoch:  123 | Loss:  1.4634 | Accuracy: 0.7400\n",
      "Epoch:  124 | Loss:  1.4558 | Accuracy: 0.7412\n",
      "Epoch:  125 | Loss:  1.4483 | Accuracy: 0.7414\n",
      "Epoch:  126 | Loss:  1.4409 | Accuracy: 0.7428\n",
      "Epoch:  127 | Loss:  1.4336 | Accuracy: 0.7438\n",
      "Epoch:  128 | Loss:  1.4265 | Accuracy: 0.7452\n",
      "Epoch:  129 | Loss:  1.4194 | Accuracy: 0.7456\n",
      "Epoch:  130 | Loss:  1.4125 | Accuracy: 0.7466\n",
      "Epoch:  131 | Loss:  1.4058 | Accuracy: 0.7478\n",
      "Epoch:  132 | Loss:  1.3991 | Accuracy: 0.7492\n",
      "Epoch:  133 | Loss:  1.3925 | Accuracy: 0.7512\n",
      "Epoch:  134 | Loss:  1.3861 | Accuracy: 0.7524\n",
      "Epoch:  135 | Loss:  1.3797 | Accuracy: 0.7528\n",
      "Epoch:  136 | Loss:  1.3735 | Accuracy: 0.7542\n",
      "Epoch:  137 | Loss:  1.3673 | Accuracy: 0.7544\n",
      "Epoch:  138 | Loss:  1.3613 | Accuracy: 0.7552\n",
      "Epoch:  139 | Loss:  1.3554 | Accuracy: 0.7564\n",
      "Epoch:  140 | Loss:  1.3495 | Accuracy: 0.7576\n",
      "Epoch:  141 | Loss:  1.3437 | Accuracy: 0.7582\n",
      "Epoch:  142 | Loss:  1.3381 | Accuracy: 0.7598\n",
      "Epoch:  143 | Loss:  1.3325 | Accuracy: 0.7606\n",
      "Epoch:  144 | Loss:  1.3270 | Accuracy: 0.7614\n",
      "Epoch:  145 | Loss:  1.3216 | Accuracy: 0.7624\n",
      "Epoch:  146 | Loss:  1.3163 | Accuracy: 0.7630\n",
      "Epoch:  147 | Loss:  1.3110 | Accuracy: 0.7638\n",
      "Epoch:  148 | Loss:  1.3059 | Accuracy: 0.7646\n",
      "Epoch:  149 | Loss:  1.3008 | Accuracy: 0.7658\n",
      "Epoch:  150 | Loss:  1.2958 | Accuracy: 0.7666\n",
      "Epoch:  151 | Loss:  1.2909 | Accuracy: 0.7674\n",
      "Epoch:  152 | Loss:  1.2860 | Accuracy: 0.7676\n",
      "Epoch:  153 | Loss:  1.2812 | Accuracy: 0.7680\n",
      "Epoch:  154 | Loss:  1.2765 | Accuracy: 0.7692\n",
      "Epoch:  155 | Loss:  1.2719 | Accuracy: 0.7714\n",
      "Epoch:  156 | Loss:  1.2673 | Accuracy: 0.7724\n",
      "Epoch:  157 | Loss:  1.2628 | Accuracy: 0.7730\n",
      "Epoch:  158 | Loss:  1.2584 | Accuracy: 0.7736\n",
      "Epoch:  159 | Loss:  1.2540 | Accuracy: 0.7750\n",
      "Epoch:  160 | Loss:  1.2497 | Accuracy: 0.7756\n",
      "Epoch:  161 | Loss:  1.2454 | Accuracy: 0.7756\n",
      "Epoch:  162 | Loss:  1.2413 | Accuracy: 0.7760\n",
      "Epoch:  163 | Loss:  1.2371 | Accuracy: 0.7766\n",
      "Epoch:  164 | Loss:  1.2331 | Accuracy: 0.7776\n",
      "Epoch:  165 | Loss:  1.2290 | Accuracy: 0.7778\n",
      "Epoch:  166 | Loss:  1.2251 | Accuracy: 0.7782\n",
      "Epoch:  167 | Loss:  1.2212 | Accuracy: 0.7784\n",
      "Epoch:  168 | Loss:  1.2173 | Accuracy: 0.7790\n",
      "Epoch:  169 | Loss:  1.2135 | Accuracy: 0.7796\n",
      "Epoch:  170 | Loss:  1.2098 | Accuracy: 0.7804\n",
      "Epoch:  171 | Loss:  1.2061 | Accuracy: 0.7816\n",
      "Epoch:  172 | Loss:  1.2025 | Accuracy: 0.7820\n",
      "Epoch:  173 | Loss:  1.1989 | Accuracy: 0.7824\n",
      "Epoch:  174 | Loss:  1.1953 | Accuracy: 0.7830\n",
      "Epoch:  175 | Loss:  1.1918 | Accuracy: 0.7830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  176 | Loss:  1.1884 | Accuracy: 0.7838\n",
      "Epoch:  177 | Loss:  1.1849 | Accuracy: 0.7840\n",
      "Epoch:  178 | Loss:  1.1816 | Accuracy: 0.7842\n",
      "Epoch:  179 | Loss:  1.1783 | Accuracy: 0.7848\n",
      "Epoch:  180 | Loss:  1.1750 | Accuracy: 0.7850\n",
      "Epoch:  181 | Loss:  1.1717 | Accuracy: 0.7852\n",
      "Epoch:  182 | Loss:  1.1685 | Accuracy: 0.7856\n",
      "Epoch:  183 | Loss:  1.1654 | Accuracy: 0.7860\n",
      "Epoch:  184 | Loss:  1.1623 | Accuracy: 0.7864\n",
      "Epoch:  185 | Loss:  1.1592 | Accuracy: 0.7866\n",
      "Epoch:  186 | Loss:  1.1561 | Accuracy: 0.7870\n",
      "Epoch:  187 | Loss:  1.1531 | Accuracy: 0.7878\n",
      "Epoch:  188 | Loss:  1.1502 | Accuracy: 0.7886\n",
      "Epoch:  189 | Loss:  1.1472 | Accuracy: 0.7894\n",
      "Epoch:  190 | Loss:  1.1443 | Accuracy: 0.7900\n",
      "Epoch:  191 | Loss:  1.1415 | Accuracy: 0.7900\n",
      "Epoch:  192 | Loss:  1.1387 | Accuracy: 0.7904\n",
      "Epoch:  193 | Loss:  1.1359 | Accuracy: 0.7906\n",
      "Epoch:  194 | Loss:  1.1331 | Accuracy: 0.7914\n",
      "Epoch:  195 | Loss:  1.1304 | Accuracy: 0.7916\n",
      "Epoch:  196 | Loss:  1.1277 | Accuracy: 0.7924\n",
      "Epoch:  197 | Loss:  1.1250 | Accuracy: 0.7930\n",
      "Epoch:  198 | Loss:  1.1224 | Accuracy: 0.7938\n",
      "Epoch:  199 | Loss:  1.1198 | Accuracy: 0.7940\n",
      "Epoch:  200 | Loss:  1.1172 | Accuracy: 0.7942\n",
      "Epoch:  201 | Loss:  1.1147 | Accuracy: 0.7948\n",
      "Epoch:  202 | Loss:  1.1122 | Accuracy: 0.7952\n",
      "Epoch:  203 | Loss:  1.1097 | Accuracy: 0.7956\n",
      "Epoch:  204 | Loss:  1.1072 | Accuracy: 0.7970\n",
      "Epoch:  205 | Loss:  1.1048 | Accuracy: 0.7974\n",
      "Epoch:  206 | Loss:  1.1024 | Accuracy: 0.7980\n",
      "Epoch:  207 | Loss:  1.1000 | Accuracy: 0.7986\n",
      "Epoch:  208 | Loss:  1.0976 | Accuracy: 0.7990\n",
      "Epoch:  209 | Loss:  1.0953 | Accuracy: 0.7990\n",
      "Epoch:  210 | Loss:  1.0930 | Accuracy: 0.7998\n",
      "Epoch:  211 | Loss:  1.0907 | Accuracy: 0.8002\n",
      "Epoch:  212 | Loss:  1.0885 | Accuracy: 0.8010\n",
      "Epoch:  213 | Loss:  1.0863 | Accuracy: 0.8012\n",
      "Epoch:  214 | Loss:  1.0841 | Accuracy: 0.8012\n",
      "Epoch:  215 | Loss:  1.0819 | Accuracy: 0.8014\n",
      "Epoch:  216 | Loss:  1.0797 | Accuracy: 0.8020\n",
      "Epoch:  217 | Loss:  1.0776 | Accuracy: 0.8022\n",
      "Epoch:  218 | Loss:  1.0755 | Accuracy: 0.8024\n",
      "Epoch:  219 | Loss:  1.0734 | Accuracy: 0.8028\n",
      "Epoch:  220 | Loss:  1.0713 | Accuracy: 0.8034\n",
      "Epoch:  221 | Loss:  1.0692 | Accuracy: 0.8036\n",
      "Epoch:  222 | Loss:  1.0672 | Accuracy: 0.8040\n",
      "Epoch:  223 | Loss:  1.0652 | Accuracy: 0.8046\n",
      "Epoch:  224 | Loss:  1.0632 | Accuracy: 0.8048\n",
      "Epoch:  225 | Loss:  1.0612 | Accuracy: 0.8050\n",
      "Epoch:  226 | Loss:  1.0593 | Accuracy: 0.8054\n",
      "Epoch:  227 | Loss:  1.0573 | Accuracy: 0.8054\n",
      "Epoch:  228 | Loss:  1.0554 | Accuracy: 0.8060\n",
      "Epoch:  229 | Loss:  1.0535 | Accuracy: 0.8062\n",
      "Epoch:  230 | Loss:  1.0516 | Accuracy: 0.8064\n",
      "Epoch:  231 | Loss:  1.0498 | Accuracy: 0.8070\n",
      "Epoch:  232 | Loss:  1.0479 | Accuracy: 0.8074\n",
      "Epoch:  233 | Loss:  1.0461 | Accuracy: 0.8080\n",
      "Epoch:  234 | Loss:  1.0443 | Accuracy: 0.8084\n",
      "Epoch:  235 | Loss:  1.0425 | Accuracy: 0.8088\n",
      "Epoch:  236 | Loss:  1.0407 | Accuracy: 0.8088\n",
      "Epoch:  237 | Loss:  1.0389 | Accuracy: 0.8092\n",
      "Epoch:  238 | Loss:  1.0372 | Accuracy: 0.8096\n",
      "Epoch:  239 | Loss:  1.0354 | Accuracy: 0.8098\n",
      "Epoch:  240 | Loss:  1.0337 | Accuracy: 0.8102\n",
      "Epoch:  241 | Loss:  1.0320 | Accuracy: 0.8104\n",
      "Epoch:  242 | Loss:  1.0303 | Accuracy: 0.8108\n",
      "Epoch:  243 | Loss:  1.0286 | Accuracy: 0.8108\n",
      "Epoch:  244 | Loss:  1.0270 | Accuracy: 0.8112\n",
      "Epoch:  245 | Loss:  1.0253 | Accuracy: 0.8114\n",
      "Epoch:  246 | Loss:  1.0237 | Accuracy: 0.8116\n",
      "Epoch:  247 | Loss:  1.0221 | Accuracy: 0.8120\n",
      "Epoch:  248 | Loss:  1.0205 | Accuracy: 0.8122\n",
      "Epoch:  249 | Loss:  1.0189 | Accuracy: 0.8122\n",
      "Epoch:  250 | Loss:  1.0173 | Accuracy: 0.8124\n",
      "Epoch:  251 | Loss:  1.0157 | Accuracy: 0.8126\n",
      "Epoch:  252 | Loss:  1.0142 | Accuracy: 0.8128\n",
      "Epoch:  253 | Loss:  1.0126 | Accuracy: 0.8130\n",
      "Epoch:  254 | Loss:  1.0111 | Accuracy: 0.8130\n",
      "Epoch:  255 | Loss:  1.0096 | Accuracy: 0.8132\n",
      "Epoch:  256 | Loss:  1.0080 | Accuracy: 0.8136\n",
      "Epoch:  257 | Loss:  1.0065 | Accuracy: 0.8136\n",
      "Epoch:  258 | Loss:  1.0051 | Accuracy: 0.8142\n",
      "Epoch:  259 | Loss:  1.0036 | Accuracy: 0.8148\n",
      "Epoch:  260 | Loss:  1.0021 | Accuracy: 0.8152\n",
      "Epoch:  261 | Loss:  1.0007 | Accuracy: 0.8156\n",
      "Epoch:  262 | Loss:  0.9992 | Accuracy: 0.8158\n",
      "Epoch:  263 | Loss:  0.9978 | Accuracy: 0.8160\n",
      "Epoch:  264 | Loss:  0.9964 | Accuracy: 0.8166\n",
      "Epoch:  265 | Loss:  0.9949 | Accuracy: 0.8168\n",
      "Epoch:  266 | Loss:  0.9935 | Accuracy: 0.8168\n",
      "Epoch:  267 | Loss:  0.9922 | Accuracy: 0.8170\n",
      "Epoch:  268 | Loss:  0.9908 | Accuracy: 0.8172\n",
      "Epoch:  269 | Loss:  0.9894 | Accuracy: 0.8176\n",
      "Epoch:  270 | Loss:  0.9880 | Accuracy: 0.8178\n",
      "Epoch:  271 | Loss:  0.9867 | Accuracy: 0.8178\n",
      "Epoch:  272 | Loss:  0.9853 | Accuracy: 0.8180\n",
      "Epoch:  273 | Loss:  0.9840 | Accuracy: 0.8180\n",
      "Epoch:  274 | Loss:  0.9827 | Accuracy: 0.8182\n",
      "Epoch:  275 | Loss:  0.9813 | Accuracy: 0.8182\n",
      "Epoch:  276 | Loss:  0.9800 | Accuracy: 0.8182\n",
      "Epoch:  277 | Loss:  0.9787 | Accuracy: 0.8182\n",
      "Epoch:  278 | Loss:  0.9774 | Accuracy: 0.8178\n",
      "Epoch:  279 | Loss:  0.9762 | Accuracy: 0.8178\n",
      "Epoch:  280 | Loss:  0.9749 | Accuracy: 0.8178\n",
      "Epoch:  281 | Loss:  0.9736 | Accuracy: 0.8184\n",
      "Epoch:  282 | Loss:  0.9724 | Accuracy: 0.8186\n",
      "Epoch:  283 | Loss:  0.9711 | Accuracy: 0.8188\n",
      "Epoch:  284 | Loss:  0.9699 | Accuracy: 0.8190\n",
      "Epoch:  285 | Loss:  0.9686 | Accuracy: 0.8194\n",
      "Epoch:  286 | Loss:  0.9674 | Accuracy: 0.8200\n",
      "Epoch:  287 | Loss:  0.9662 | Accuracy: 0.8202\n",
      "Epoch:  288 | Loss:  0.9650 | Accuracy: 0.8204\n",
      "Epoch:  289 | Loss:  0.9638 | Accuracy: 0.8206\n",
      "Epoch:  290 | Loss:  0.9626 | Accuracy: 0.8204\n",
      "Epoch:  291 | Loss:  0.9614 | Accuracy: 0.8208\n",
      "Epoch:  292 | Loss:  0.9602 | Accuracy: 0.8210\n",
      "Epoch:  293 | Loss:  0.9590 | Accuracy: 0.8212\n",
      "Epoch:  294 | Loss:  0.9578 | Accuracy: 0.8216\n",
      "Epoch:  295 | Loss:  0.9567 | Accuracy: 0.8220\n",
      "Epoch:  296 | Loss:  0.9555 | Accuracy: 0.8224\n",
      "Epoch:  297 | Loss:  0.9544 | Accuracy: 0.8226\n",
      "Epoch:  298 | Loss:  0.9532 | Accuracy: 0.8236\n",
      "Epoch:  299 | Loss:  0.9521 | Accuracy: 0.8236\n",
      "Epoch:  300 | Loss:  0.9509 | Accuracy: 0.8236\n",
      "Epoch:  301 | Loss:  0.9498 | Accuracy: 0.8244\n",
      "Epoch:  302 | Loss:  0.9487 | Accuracy: 0.8246\n",
      "Epoch:  303 | Loss:  0.9476 | Accuracy: 0.8248\n",
      "Epoch:  304 | Loss:  0.9465 | Accuracy: 0.8248\n",
      "Epoch:  305 | Loss:  0.9454 | Accuracy: 0.8250\n",
      "Epoch:  306 | Loss:  0.9443 | Accuracy: 0.8252\n",
      "Epoch:  307 | Loss:  0.9432 | Accuracy: 0.8252\n",
      "Epoch:  308 | Loss:  0.9421 | Accuracy: 0.8254\n",
      "Epoch:  309 | Loss:  0.9410 | Accuracy: 0.8262\n",
      "Epoch:  310 | Loss:  0.9400 | Accuracy: 0.8264\n",
      "Epoch:  311 | Loss:  0.9389 | Accuracy: 0.8266\n",
      "Epoch:  312 | Loss:  0.9378 | Accuracy: 0.8270\n",
      "Epoch:  313 | Loss:  0.9368 | Accuracy: 0.8272\n",
      "Epoch:  314 | Loss:  0.9357 | Accuracy: 0.8270\n",
      "Epoch:  315 | Loss:  0.9347 | Accuracy: 0.8276\n",
      "Epoch:  316 | Loss:  0.9336 | Accuracy: 0.8276\n",
      "Epoch:  317 | Loss:  0.9326 | Accuracy: 0.8280\n",
      "Epoch:  318 | Loss:  0.9316 | Accuracy: 0.8282\n",
      "Epoch:  319 | Loss:  0.9306 | Accuracy: 0.8284\n",
      "Epoch:  320 | Loss:  0.9295 | Accuracy: 0.8286\n",
      "Epoch:  321 | Loss:  0.9285 | Accuracy: 0.8286\n",
      "Epoch:  322 | Loss:  0.9275 | Accuracy: 0.8290\n",
      "Epoch:  323 | Loss:  0.9265 | Accuracy: 0.8290\n",
      "Epoch:  324 | Loss:  0.9255 | Accuracy: 0.8294\n",
      "Epoch:  325 | Loss:  0.9245 | Accuracy: 0.8294\n",
      "Epoch:  326 | Loss:  0.9235 | Accuracy: 0.8294\n",
      "Epoch:  327 | Loss:  0.9226 | Accuracy: 0.8294\n",
      "Epoch:  328 | Loss:  0.9216 | Accuracy: 0.8294\n",
      "Epoch:  329 | Loss:  0.9206 | Accuracy: 0.8294\n",
      "Epoch:  330 | Loss:  0.9196 | Accuracy: 0.8294\n",
      "Epoch:  331 | Loss:  0.9187 | Accuracy: 0.8294\n",
      "Epoch:  332 | Loss:  0.9177 | Accuracy: 0.8296\n",
      "Epoch:  333 | Loss:  0.9167 | Accuracy: 0.8298\n",
      "Epoch:  334 | Loss:  0.9158 | Accuracy: 0.8298\n",
      "Epoch:  335 | Loss:  0.9148 | Accuracy: 0.8296\n",
      "Epoch:  336 | Loss:  0.9139 | Accuracy: 0.8296\n",
      "Epoch:  337 | Loss:  0.9130 | Accuracy: 0.8296\n",
      "Epoch:  338 | Loss:  0.9120 | Accuracy: 0.8296\n",
      "Epoch:  339 | Loss:  0.9111 | Accuracy: 0.8294\n",
      "Epoch:  340 | Loss:  0.9102 | Accuracy: 0.8296\n",
      "Epoch:  341 | Loss:  0.9093 | Accuracy: 0.8302\n",
      "Epoch:  342 | Loss:  0.9083 | Accuracy: 0.8304\n",
      "Epoch:  343 | Loss:  0.9074 | Accuracy: 0.8304\n",
      "Epoch:  344 | Loss:  0.9065 | Accuracy: 0.8306\n",
      "Epoch:  345 | Loss:  0.9056 | Accuracy: 0.8306\n",
      "Epoch:  346 | Loss:  0.9047 | Accuracy: 0.8308\n",
      "Epoch:  347 | Loss:  0.9038 | Accuracy: 0.8308\n",
      "Epoch:  348 | Loss:  0.9029 | Accuracy: 0.8308\n",
      "Epoch:  349 | Loss:  0.9020 | Accuracy: 0.8310\n",
      "Epoch:  350 | Loss:  0.9011 | Accuracy: 0.8310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  351 | Loss:  0.9002 | Accuracy: 0.8314\n",
      "Epoch:  352 | Loss:  0.8994 | Accuracy: 0.8314\n",
      "Epoch:  353 | Loss:  0.8985 | Accuracy: 0.8314\n",
      "Epoch:  354 | Loss:  0.8976 | Accuracy: 0.8314\n",
      "Epoch:  355 | Loss:  0.8968 | Accuracy: 0.8314\n",
      "Epoch:  356 | Loss:  0.8959 | Accuracy: 0.8316\n",
      "Epoch:  357 | Loss:  0.8950 | Accuracy: 0.8316\n",
      "Epoch:  358 | Loss:  0.8942 | Accuracy: 0.8316\n",
      "Epoch:  359 | Loss:  0.8933 | Accuracy: 0.8316\n",
      "Epoch:  360 | Loss:  0.8925 | Accuracy: 0.8318\n",
      "Epoch:  361 | Loss:  0.8916 | Accuracy: 0.8318\n",
      "Epoch:  362 | Loss:  0.8908 | Accuracy: 0.8322\n",
      "Epoch:  363 | Loss:  0.8899 | Accuracy: 0.8322\n",
      "Epoch:  364 | Loss:  0.8891 | Accuracy: 0.8324\n",
      "Epoch:  365 | Loss:  0.8883 | Accuracy: 0.8326\n",
      "Epoch:  366 | Loss:  0.8874 | Accuracy: 0.8326\n",
      "Epoch:  367 | Loss:  0.8866 | Accuracy: 0.8330\n",
      "Epoch:  368 | Loss:  0.8858 | Accuracy: 0.8330\n",
      "Epoch:  369 | Loss:  0.8850 | Accuracy: 0.8336\n",
      "Epoch:  370 | Loss:  0.8841 | Accuracy: 0.8338\n",
      "Epoch:  371 | Loss:  0.8833 | Accuracy: 0.8342\n",
      "Epoch:  372 | Loss:  0.8825 | Accuracy: 0.8340\n",
      "Epoch:  373 | Loss:  0.8817 | Accuracy: 0.8346\n",
      "Epoch:  374 | Loss:  0.8809 | Accuracy: 0.8346\n",
      "Epoch:  375 | Loss:  0.8801 | Accuracy: 0.8346\n",
      "Epoch:  376 | Loss:  0.8793 | Accuracy: 0.8346\n",
      "Epoch:  377 | Loss:  0.8785 | Accuracy: 0.8348\n",
      "Epoch:  378 | Loss:  0.8777 | Accuracy: 0.8348\n",
      "Epoch:  379 | Loss:  0.8769 | Accuracy: 0.8348\n",
      "Epoch:  380 | Loss:  0.8761 | Accuracy: 0.8354\n",
      "Epoch:  381 | Loss:  0.8753 | Accuracy: 0.8354\n",
      "Epoch:  382 | Loss:  0.8746 | Accuracy: 0.8354\n",
      "Epoch:  383 | Loss:  0.8738 | Accuracy: 0.8356\n",
      "Epoch:  384 | Loss:  0.8730 | Accuracy: 0.8360\n",
      "Epoch:  385 | Loss:  0.8722 | Accuracy: 0.8360\n",
      "Epoch:  386 | Loss:  0.8715 | Accuracy: 0.8360\n",
      "Epoch:  387 | Loss:  0.8707 | Accuracy: 0.8360\n",
      "Epoch:  388 | Loss:  0.8699 | Accuracy: 0.8360\n",
      "Epoch:  389 | Loss:  0.8692 | Accuracy: 0.8360\n",
      "Epoch:  390 | Loss:  0.8684 | Accuracy: 0.8358\n",
      "Epoch:  391 | Loss:  0.8676 | Accuracy: 0.8360\n",
      "Epoch:  392 | Loss:  0.8669 | Accuracy: 0.8364\n",
      "Epoch:  393 | Loss:  0.8661 | Accuracy: 0.8364\n",
      "Epoch:  394 | Loss:  0.8654 | Accuracy: 0.8364\n",
      "Epoch:  395 | Loss:  0.8646 | Accuracy: 0.8370\n",
      "Epoch:  396 | Loss:  0.8639 | Accuracy: 0.8372\n",
      "Epoch:  397 | Loss:  0.8632 | Accuracy: 0.8374\n",
      "Epoch:  398 | Loss:  0.8624 | Accuracy: 0.8374\n",
      "Epoch:  399 | Loss:  0.8617 | Accuracy: 0.8374\n",
      "Epoch:  400 | Loss:  0.8609 | Accuracy: 0.8374\n",
      "Epoch:  401 | Loss:  0.8602 | Accuracy: 0.8376\n",
      "Epoch:  402 | Loss:  0.8595 | Accuracy: 0.8376\n",
      "Epoch:  403 | Loss:  0.8588 | Accuracy: 0.8376\n",
      "Epoch:  404 | Loss:  0.8580 | Accuracy: 0.8376\n",
      "Epoch:  405 | Loss:  0.8573 | Accuracy: 0.8376\n",
      "Epoch:  406 | Loss:  0.8566 | Accuracy: 0.8382\n",
      "Epoch:  407 | Loss:  0.8559 | Accuracy: 0.8384\n",
      "Epoch:  408 | Loss:  0.8552 | Accuracy: 0.8386\n",
      "Epoch:  409 | Loss:  0.8545 | Accuracy: 0.8386\n",
      "Epoch:  410 | Loss:  0.8537 | Accuracy: 0.8394\n",
      "Epoch:  411 | Loss:  0.8530 | Accuracy: 0.8394\n",
      "Epoch:  412 | Loss:  0.8523 | Accuracy: 0.8396\n",
      "Epoch:  413 | Loss:  0.8516 | Accuracy: 0.8396\n",
      "Epoch:  414 | Loss:  0.8509 | Accuracy: 0.8396\n",
      "Epoch:  415 | Loss:  0.8502 | Accuracy: 0.8396\n",
      "Epoch:  416 | Loss:  0.8495 | Accuracy: 0.8398\n",
      "Epoch:  417 | Loss:  0.8488 | Accuracy: 0.8398\n",
      "Epoch:  418 | Loss:  0.8481 | Accuracy: 0.8400\n",
      "Epoch:  419 | Loss:  0.8475 | Accuracy: 0.8404\n",
      "Epoch:  420 | Loss:  0.8468 | Accuracy: 0.8404\n",
      "Epoch:  421 | Loss:  0.8461 | Accuracy: 0.8404\n",
      "Epoch:  422 | Loss:  0.8454 | Accuracy: 0.8402\n",
      "Epoch:  423 | Loss:  0.8447 | Accuracy: 0.8400\n",
      "Epoch:  424 | Loss:  0.8440 | Accuracy: 0.8402\n",
      "Epoch:  425 | Loss:  0.8434 | Accuracy: 0.8402\n",
      "Epoch:  426 | Loss:  0.8427 | Accuracy: 0.8404\n",
      "Epoch:  427 | Loss:  0.8420 | Accuracy: 0.8406\n",
      "Epoch:  428 | Loss:  0.8413 | Accuracy: 0.8412\n",
      "Epoch:  429 | Loss:  0.8407 | Accuracy: 0.8412\n",
      "Epoch:  430 | Loss:  0.8400 | Accuracy: 0.8412\n",
      "Epoch:  431 | Loss:  0.8393 | Accuracy: 0.8414\n",
      "Epoch:  432 | Loss:  0.8387 | Accuracy: 0.8414\n",
      "Epoch:  433 | Loss:  0.8380 | Accuracy: 0.8414\n",
      "Epoch:  434 | Loss:  0.8374 | Accuracy: 0.8414\n",
      "Epoch:  435 | Loss:  0.8367 | Accuracy: 0.8414\n",
      "Epoch:  436 | Loss:  0.8361 | Accuracy: 0.8414\n",
      "Epoch:  437 | Loss:  0.8354 | Accuracy: 0.8414\n",
      "Epoch:  438 | Loss:  0.8348 | Accuracy: 0.8414\n",
      "Epoch:  439 | Loss:  0.8341 | Accuracy: 0.8414\n",
      "Epoch:  440 | Loss:  0.8335 | Accuracy: 0.8412\n",
      "Epoch:  441 | Loss:  0.8328 | Accuracy: 0.8412\n",
      "Epoch:  442 | Loss:  0.8322 | Accuracy: 0.8416\n",
      "Epoch:  443 | Loss:  0.8315 | Accuracy: 0.8420\n",
      "Epoch:  444 | Loss:  0.8309 | Accuracy: 0.8420\n",
      "Epoch:  445 | Loss:  0.8303 | Accuracy: 0.8422\n",
      "Epoch:  446 | Loss:  0.8296 | Accuracy: 0.8424\n",
      "Epoch:  447 | Loss:  0.8290 | Accuracy: 0.8424\n",
      "Epoch:  448 | Loss:  0.8284 | Accuracy: 0.8424\n",
      "Epoch:  449 | Loss:  0.8277 | Accuracy: 0.8424\n",
      "Epoch:  450 | Loss:  0.8271 | Accuracy: 0.8424\n",
      "Epoch:  451 | Loss:  0.8265 | Accuracy: 0.8424\n",
      "Epoch:  452 | Loss:  0.8258 | Accuracy: 0.8426\n",
      "Epoch:  453 | Loss:  0.8252 | Accuracy: 0.8426\n",
      "Epoch:  454 | Loss:  0.8246 | Accuracy: 0.8426\n",
      "Epoch:  455 | Loss:  0.8240 | Accuracy: 0.8426\n",
      "Epoch:  456 | Loss:  0.8234 | Accuracy: 0.8426\n",
      "Epoch:  457 | Loss:  0.8228 | Accuracy: 0.8426\n",
      "Epoch:  458 | Loss:  0.8221 | Accuracy: 0.8426\n",
      "Epoch:  459 | Loss:  0.8215 | Accuracy: 0.8426\n",
      "Epoch:  460 | Loss:  0.8209 | Accuracy: 0.8430\n",
      "Epoch:  461 | Loss:  0.8203 | Accuracy: 0.8428\n",
      "Epoch:  462 | Loss:  0.8197 | Accuracy: 0.8428\n",
      "Epoch:  463 | Loss:  0.8191 | Accuracy: 0.8432\n",
      "Epoch:  464 | Loss:  0.8185 | Accuracy: 0.8432\n",
      "Epoch:  465 | Loss:  0.8179 | Accuracy: 0.8432\n",
      "Epoch:  466 | Loss:  0.8173 | Accuracy: 0.8432\n",
      "Epoch:  467 | Loss:  0.8167 | Accuracy: 0.8434\n",
      "Epoch:  468 | Loss:  0.8161 | Accuracy: 0.8434\n",
      "Epoch:  469 | Loss:  0.8155 | Accuracy: 0.8434\n",
      "Epoch:  470 | Loss:  0.8149 | Accuracy: 0.8436\n",
      "Epoch:  471 | Loss:  0.8143 | Accuracy: 0.8436\n",
      "Epoch:  472 | Loss:  0.8137 | Accuracy: 0.8434\n",
      "Epoch:  473 | Loss:  0.8131 | Accuracy: 0.8432\n",
      "Epoch:  474 | Loss:  0.8126 | Accuracy: 0.8434\n",
      "Epoch:  475 | Loss:  0.8120 | Accuracy: 0.8434\n",
      "Epoch:  476 | Loss:  0.8114 | Accuracy: 0.8436\n",
      "Epoch:  477 | Loss:  0.8108 | Accuracy: 0.8436\n",
      "Epoch:  478 | Loss:  0.8102 | Accuracy: 0.8436\n",
      "Epoch:  479 | Loss:  0.8096 | Accuracy: 0.8436\n",
      "Epoch:  480 | Loss:  0.8091 | Accuracy: 0.8436\n",
      "Epoch:  481 | Loss:  0.8085 | Accuracy: 0.8438\n",
      "Epoch:  482 | Loss:  0.8079 | Accuracy: 0.8438\n",
      "Epoch:  483 | Loss:  0.8073 | Accuracy: 0.8440\n",
      "Epoch:  484 | Loss:  0.8068 | Accuracy: 0.8442\n",
      "Epoch:  485 | Loss:  0.8062 | Accuracy: 0.8442\n",
      "Epoch:  486 | Loss:  0.8056 | Accuracy: 0.8444\n",
      "Epoch:  487 | Loss:  0.8051 | Accuracy: 0.8444\n",
      "Epoch:  488 | Loss:  0.8045 | Accuracy: 0.8444\n",
      "Epoch:  489 | Loss:  0.8039 | Accuracy: 0.8442\n",
      "Epoch:  490 | Loss:  0.8034 | Accuracy: 0.8442\n",
      "Epoch:  491 | Loss:  0.8028 | Accuracy: 0.8442\n",
      "Epoch:  492 | Loss:  0.8022 | Accuracy: 0.8442\n",
      "Epoch:  493 | Loss:  0.8017 | Accuracy: 0.8442\n",
      "Epoch:  494 | Loss:  0.8011 | Accuracy: 0.8442\n",
      "Epoch:  495 | Loss:  0.8006 | Accuracy: 0.8446\n",
      "Epoch:  496 | Loss:  0.8000 | Accuracy: 0.8446\n",
      "Epoch:  497 | Loss:  0.7995 | Accuracy: 0.8448\n",
      "Epoch:  498 | Loss:  0.7989 | Accuracy: 0.8450\n",
      "Epoch:  499 | Loss:  0.7984 | Accuracy: 0.8450\n",
      "Epoch:  500 | Loss:  0.7978 | Accuracy: 0.8450\n",
      "Test accuracy: 0.8533\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "batches = batches(batch_size, train_features, train_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_features, batch_labels in batches:\n",
    "            sess.run(optimizer, feed_dict={features: batch_features,\n",
    "                                           labels: batch_labels,\n",
    "                                           learning_rate: lr})\n",
    "        print_epoch_stats(epoch, sess, batch_features, batch_labels)\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: test_features,\n",
    "                                                  labels: test_labels})\n",
    "\n",
    "print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A acurácia alcançou 0.85, mas não alteramos em nenhum momento a taxa de aprendizado. Aumentar a taxa de aprendizado aumentaria a velocidade com que o modelo aprende (diminuindo a quantidade de épocas necessárias), mas poderia prender o modelo em algum mínimo local. Por outro lado, diminuir a taxa de aprendizado tornaria o aprendizado mais lento (demandando mais épocas de treinamento), mas poderia resultar em um melhor desempenho no final.\n",
    "\n",
    "No notebook `not_mnist_with_tf.ipynb`, você terá a oportunidade de escolher os hiperparâmetros a fim de melhorar o desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Salvando e recuperando modelos do TensorFlow\n",
    "\n",
    "Treinar um modelo pode levar horas, mas quando você encerra uma sessão do tensorFlow todos os pesos e vieses treinados são perdidos. Se você tivesse que reutilizar o modelo no futuro, seria preciso treinar tudo de novo.\n",
    "\n",
    "Felizmente, o TensorFlow te dá a possibilidade de salvar o seu progresso usando uma classe chamada [**`tf.train.Saver`**](https://www.tensorflow.org/api_docs/python/tf/train/Saver). Essa classe fornece a funcionalildade de salvar qualquer [**`tf.Variable`**](https://www.tensorflow.org/api_docs/python/tf/Variable) para o seu sistema de arquivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando variáveis\n",
    "\n",
    "Vamos começar com um exemplo simples, salvando apenas as variáveis `weights` e `bias`. Exemplos futuros vão salvar todos os pesos em um modelo prático."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos:\n",
      "[[ 0.73610044  0.01878193 -0.20724149]\n",
      " [ 0.6261984  -1.2588857   0.6716363 ]]\n",
      "Vieses:\n",
      "[ 0.79500794 -0.70658654  0.7468536 ]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# O caminho do arquivo em que os dados devem ser salvos\n",
    "save_file = './model.ckpt'\n",
    "\n",
    "# Duas variáveis do TensorFlow\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Classe usada para salvar e/ou recuperar variáveis no TensorFlow\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Pesos:')\n",
    "    print(sess.run(weights))\n",
    "    print('Vieses:')\n",
    "    print(sess.run(bias))\n",
    "    saver.save(sess, save_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os tensores `weights` e `bias` são inicializados com valores aleatórios usando a função [**`tf.truncated_normal()`**](https://www.tensorflow.org/api_docs/python/tf/truncated_normal). Em seguida, os valores são salvos na localização indicada pela variável `save_file`, 'model.ckpt', usando a função[**`tf.train.Saver.save()`**](https://www.tensorflow.org/api_docs/python/tf/train/Saver#save). A extensão .ckpt é uma abreviação da palavra \"checkpoint\".\n",
    "\n",
    "Se você está usando o TensorFlow 0.11.0RC1 ou mais recente, um arquivo chamado model.ckpt.meta também será criado. Esse arquivo contém o grafo do TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando variáveis\n",
    "\n",
    "Agora que as variáveis do TensorFlow foram salvas, vamos carregá-las em um novo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model.ckpt\n",
      "Pesos:\n",
      "[[ 0.73610044  0.01878193 -0.20724149]\n",
      " [ 0.6261984  -1.2588857   0.6716363 ]]\n",
      "Vieses:\n",
      "[ 0.79500794 -0.70658654  0.7468536 ]\n"
     ]
    }
   ],
   "source": [
    "# Remover os pesos e vieses anteriores\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Duas variáveis do TensorFlow\n",
    "weights = tf.Variable(tf.truncated_normal([2, 3]))\n",
    "bias = tf.Variable(tf.truncated_normal([3]))\n",
    "\n",
    "# Classe usada para salvar e/ou recuperar variáveis no TensorFlow\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    print('Pesos:')\n",
    "    print(sess.run(weights))\n",
    "    print('Vieses:')\n",
    "    print(sess.run(bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você deve ter notado que ainda é preciso criar os tensores `weights` e `bias`. A função [**`tf.train.Saver.restore()`**](https://www.tensorflow.org/api_docs/python/tf/train/Saver#restore) carrega os dados salvos nas variáveis `weigthts` e `bias`. Como esta função inicializa todas as variáveis do TensorFlow, não é necessário chamar a função [**`tf.global_variables_initializer()`**](https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando um modelo treinado\n",
    "\n",
    "Nesta seção, vamos ver como treinar um modelo e salvar seus pesos. Primeiro, precisamos iniciar o modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.001\n",
    "n_input = 28 * 28\n",
    "n_classes = 10\n",
    "\n",
    "mnist = input_data.read_data_sets('.', one_hot=True)\n",
    "\n",
    "features = tf.placeholder(tf.float32, [None, n_input])\n",
    "labels = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([n_input, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora treinar o modelo e depois salvar os pesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1 | Loss: 12.4094 | Accuracy: 0.1454\n",
      "Epoch:    2 | Loss: 10.2257 | Accuracy: 0.1618\n",
      "Epoch:    3 | Loss:  9.7524 | Accuracy: 0.1740\n",
      "Epoch:    4 | Loss: 10.0111 | Accuracy: 0.1896\n",
      "Epoch:    5 | Loss:  7.8830 | Accuracy: 0.2050\n",
      "Epoch:    6 | Loss:  7.4530 | Accuracy: 0.2204\n",
      "Epoch:    7 | Loss:  6.9032 | Accuracy: 0.2356\n",
      "Epoch:    8 | Loss:  7.1567 | Accuracy: 0.2470\n",
      "Epoch:    9 | Loss:  7.7237 | Accuracy: 0.2612\n",
      "Epoch:   10 | Loss:  6.8139 | Accuracy: 0.2774\n",
      "Epoch:   11 | Loss:  6.3200 | Accuracy: 0.2934\n",
      "Epoch:   12 | Loss:  6.1303 | Accuracy: 0.3062\n",
      "Epoch:   13 | Loss:  6.0899 | Accuracy: 0.3232\n",
      "Epoch:   14 | Loss:  6.0223 | Accuracy: 0.3350\n",
      "Epoch:   15 | Loss:  5.2688 | Accuracy: 0.3492\n",
      "Epoch:   16 | Loss:  5.0779 | Accuracy: 0.3618\n",
      "Epoch:   17 | Loss:  4.3600 | Accuracy: 0.3764\n",
      "Epoch:   18 | Loss:  5.1524 | Accuracy: 0.3894\n",
      "Epoch:   19 | Loss:  4.3399 | Accuracy: 0.4028\n",
      "Epoch:   20 | Loss:  3.9217 | Accuracy: 0.4156\n",
      "Epoch:   21 | Loss:  5.4073 | Accuracy: 0.4256\n",
      "Epoch:   22 | Loss:  4.0796 | Accuracy: 0.4352\n",
      "Epoch:   23 | Loss:  3.4779 | Accuracy: 0.4450\n",
      "Epoch:   24 | Loss:  3.4769 | Accuracy: 0.4570\n",
      "Epoch:   25 | Loss:  3.8971 | Accuracy: 0.4650\n",
      "Epoch:   26 | Loss:  3.4281 | Accuracy: 0.4752\n",
      "Epoch:   27 | Loss:  3.6249 | Accuracy: 0.4840\n",
      "Epoch:   28 | Loss:  2.8460 | Accuracy: 0.4946\n",
      "Epoch:   29 | Loss:  3.0621 | Accuracy: 0.5020\n",
      "Epoch:   30 | Loss:  3.3600 | Accuracy: 0.5116\n",
      "Epoch:   31 | Loss:  3.1402 | Accuracy: 0.5214\n",
      "Epoch:   32 | Loss:  2.9715 | Accuracy: 0.5302\n",
      "Epoch:   33 | Loss:  3.3446 | Accuracy: 0.5370\n",
      "Epoch:   34 | Loss:  2.4311 | Accuracy: 0.5424\n",
      "Epoch:   35 | Loss:  3.4018 | Accuracy: 0.5486\n",
      "Epoch:   36 | Loss:  3.5029 | Accuracy: 0.5556\n",
      "Epoch:   37 | Loss:  2.3939 | Accuracy: 0.5608\n",
      "Epoch:   38 | Loss:  2.9920 | Accuracy: 0.5678\n",
      "Epoch:   39 | Loss:  3.3056 | Accuracy: 0.5724\n",
      "Epoch:   40 | Loss:  2.3039 | Accuracy: 0.5770\n",
      "Epoch:   41 | Loss:  2.6669 | Accuracy: 0.5834\n",
      "Epoch:   42 | Loss:  3.2087 | Accuracy: 0.5890\n",
      "Epoch:   43 | Loss:  2.8573 | Accuracy: 0.5942\n",
      "Epoch:   44 | Loss:  2.4970 | Accuracy: 0.5980\n",
      "Epoch:   45 | Loss:  2.3747 | Accuracy: 0.6016\n",
      "Epoch:   46 | Loss:  2.5028 | Accuracy: 0.6074\n",
      "Epoch:   47 | Loss:  2.1670 | Accuracy: 0.6128\n",
      "Epoch:   48 | Loss:  3.5029 | Accuracy: 0.6170\n",
      "Epoch:   49 | Loss:  2.7668 | Accuracy: 0.6216\n",
      "Epoch:   50 | Loss:  2.4888 | Accuracy: 0.6254\n",
      "Epoch:   51 | Loss:  1.9817 | Accuracy: 0.6290\n",
      "Epoch:   52 | Loss:  2.1624 | Accuracy: 0.6316\n",
      "Epoch:   53 | Loss:  2.8074 | Accuracy: 0.6344\n",
      "Epoch:   54 | Loss:  2.0279 | Accuracy: 0.6372\n",
      "Epoch:   55 | Loss:  2.0217 | Accuracy: 0.6408\n",
      "Epoch:   56 | Loss:  2.4413 | Accuracy: 0.6452\n",
      "Epoch:   57 | Loss:  1.9948 | Accuracy: 0.6486\n",
      "Epoch:   58 | Loss:  1.8342 | Accuracy: 0.6520\n",
      "Epoch:   59 | Loss:  1.6772 | Accuracy: 0.6554\n",
      "Epoch:   60 | Loss:  2.2234 | Accuracy: 0.6590\n",
      "Epoch:   61 | Loss:  1.6045 | Accuracy: 0.6630\n",
      "Epoch:   62 | Loss:  2.3554 | Accuracy: 0.6658\n",
      "Epoch:   63 | Loss:  1.2814 | Accuracy: 0.6694\n",
      "Epoch:   64 | Loss:  2.2079 | Accuracy: 0.6710\n",
      "Epoch:   65 | Loss:  2.0976 | Accuracy: 0.6730\n",
      "Epoch:   66 | Loss:  1.2175 | Accuracy: 0.6748\n",
      "Epoch:   67 | Loss:  1.8907 | Accuracy: 0.6758\n",
      "Epoch:   68 | Loss:  1.9670 | Accuracy: 0.6776\n",
      "Epoch:   69 | Loss:  2.2144 | Accuracy: 0.6792\n",
      "Epoch:   70 | Loss:  1.7774 | Accuracy: 0.6804\n",
      "Epoch:   71 | Loss:  1.7719 | Accuracy: 0.6822\n",
      "Epoch:   72 | Loss:  1.8865 | Accuracy: 0.6842\n",
      "Epoch:   73 | Loss:  2.1459 | Accuracy: 0.6860\n",
      "Epoch:   74 | Loss:  2.0238 | Accuracy: 0.6894\n",
      "Epoch:   75 | Loss:  1.4238 | Accuracy: 0.6916\n",
      "Epoch:   76 | Loss:  1.6423 | Accuracy: 0.6932\n",
      "Epoch:   77 | Loss:  1.6282 | Accuracy: 0.6954\n",
      "Epoch:   78 | Loss:  1.8431 | Accuracy: 0.6970\n",
      "Epoch:   79 | Loss:  2.2091 | Accuracy: 0.6988\n",
      "Epoch:   80 | Loss:  1.6083 | Accuracy: 0.7002\n",
      "Epoch:   81 | Loss:  1.6193 | Accuracy: 0.7024\n",
      "Epoch:   82 | Loss:  1.8823 | Accuracy: 0.7048\n",
      "Epoch:   83 | Loss:  1.3786 | Accuracy: 0.7072\n",
      "Epoch:   84 | Loss:  1.8791 | Accuracy: 0.7098\n",
      "Epoch:   85 | Loss:  1.4420 | Accuracy: 0.7118\n",
      "Epoch:   86 | Loss:  1.5858 | Accuracy: 0.7134\n",
      "Epoch:   87 | Loss:  1.8363 | Accuracy: 0.7146\n",
      "Epoch:   88 | Loss:  1.2165 | Accuracy: 0.7156\n",
      "Epoch:   89 | Loss:  1.5906 | Accuracy: 0.7166\n",
      "Epoch:   90 | Loss:  1.3506 | Accuracy: 0.7178\n",
      "Epoch:   91 | Loss:  1.5802 | Accuracy: 0.7184\n",
      "Epoch:   92 | Loss:  1.1746 | Accuracy: 0.7194\n",
      "Epoch:   93 | Loss:  1.7867 | Accuracy: 0.7206\n",
      "Epoch:   94 | Loss:  1.3987 | Accuracy: 0.7222\n",
      "Epoch:   95 | Loss:  1.9727 | Accuracy: 0.7234\n",
      "Epoch:   96 | Loss:  1.6910 | Accuracy: 0.7250\n",
      "Epoch:   97 | Loss:  2.1996 | Accuracy: 0.7266\n",
      "Epoch:   98 | Loss:  1.1786 | Accuracy: 0.7288\n",
      "Epoch:   99 | Loss:  1.4437 | Accuracy: 0.7298\n",
      "Epoch:  100 | Loss:  1.1577 | Accuracy: 0.7316\n",
      "Trained model saved.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "save_file = './trained_model.ckpt'\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        total_batch = math.ceil(mnist.train.num_examples / batch_size)\n",
    "        for _ in range(total_batch):\n",
    "            batch_features, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={features: batch_features,\n",
    "                                           labels: batch_labels})\n",
    "            current_loss = sess.run(loss, feed_dict={features: batch_features, \n",
    "                                                 labels: batch_labels})\n",
    "        validation_accuracy = sess.run(accuracy, feed_dict={features: mnist.validation.images,\n",
    "                                                            labels: mnist.validation.labels})\n",
    "        print('Epoch: {:4d} | Loss: {:7.4f} | Accuracy: {:.4f}'.format(epoch + 1, current_loss, validation_accuracy))\n",
    "\n",
    "    saver.save(sess, save_file)\n",
    "    print('Trained model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando um modelo treinado\n",
    "\n",
    "Vamos agora carregar os pesos e vieses da memória, para então verificar a acurácia de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained_model.ckpt\n",
      "Test accuracy: 0.7264\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "    \n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features: mnist.test.images,\n",
    "                                                  labels: mnist.test.labels})\n",
    "\n",
    "print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

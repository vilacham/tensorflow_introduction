{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neurais profundas\n",
    "\n",
    "Nos notebooks anteriores (`introducao.ipynb` e `not_mnist_with_tf.ipynb`) você aprendeu como construir uma rede neural com uma camada. Neste notebook você vai aprender a construir uma rede neural de várias camadas com o TensorFlow. Adicionar uma camada oculta a uma rede permite que ela modele funções mais complexas. Além disso, usar uma função de ativação não linear na camada oculta permite que ela modele funções não lineares.\n",
    "\n",
    "Este notebook foi construído a partir dos conhecimentos saprensentados na aula `Introdução às redes neurais`. Conceitos como feedforward e dropout são alguns dos muitos cruciais para o entendimento completo deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ReLu\n",
    "\n",
    "<img src='img/two_layer_network.png' width=600px>\n",
    "\n",
    "A primeira coisa que nós vamos aprender a implementar no TensorFlow é a camada oculta com função de ativação ReLu (Rectified Linear Unit, ou unidade retificada linear).\n",
    "\n",
    "A ReLu é uma função não linear que retorna 0 para entradas negativas e $x$ para todas as entradas $x > 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neurais profundas\n",
    "\n",
    "Nos notebooks anteriores (`introducao.ipynb` e `not_mnist_with_tf.ipynb`) você aprendeu como construir uma rede neural com uma camada. Neste notebook você vai aprender a construir uma rede neural de várias camadas com o TensorFlow. Adicionar uma camada oculta a uma rede permite que ela modele funções mais complexas. Além disso, usar uma função de ativação não linear na camada oculta permite que ela modele funções não lineares.\n",
    "\n",
    "Este notebook foi construído a partir dos conhecimentos saprensentados na aula `Introdução às redes neurais`. Conceitos como feedforward e dropout são alguns dos muitos cruciais para o entendimento completo deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ReLu\n",
    "\n",
    "A primeira coisa que nós vamos aprender a implementar no TensorFlow é a camada oculta com função de ativação ReLu (Rectified Linear Unit, ou unidade retificada linear). A ReLu é uma função não linear que retorna 0 para entradas negativas e $x$ para todas as entradas $x > 0$.\n",
    "\n",
    "O TensorFlow fornece a função ReLu como [**`tf.nn.relu()`**](https://www.tensorflow.org/api_docs/python/tf/nn/relu), como visto no trecho de código abaixo:\n",
    "\n",
    "```\n",
    "# Camada oculta com função de ativação ReLu\n",
    "hidden_layer = tf.add(tf.matmul(X, hidden_w), hidden_b)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_w), output_b)\n",
    "```\n",
    "\n",
    "O trecho acima aplica a função [**`tf.nn.relu()`**](https://www.tensorflow.org/api_docs/python/tf/nn/relu) na camada `hidden_layer`, o que ba prática desativa quaisquer pesos negativos e funciona como chave liga/desliga. Camadas adicionais, como a camada `output`, aplicadas após a ativação transformam o modelo em uma função não-linear. Essa não-linearidade permite que a rede resolva problemas mais complexos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementação\n",
    "\n",
    "<img src='img/two_layer_network.png' width=600px>\n",
    "\n",
    "\n",
    "Na célula abaixo, você vai usar a função ReLu para transformar uma rede linear de uma camada em uma rede não-linear multicamadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.11      8.440001]\n",
      " [ 0.        0.      ]\n",
      " [24.010002 38.239998]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [[0.1, 0.2, 0.4],\n",
    "                        [0.4, 0.6, 0.6],\n",
    "                        [0.5, 0.9, 0.1],\n",
    "                        [0.8, 0.2, 0.8]]\n",
    "out_weights = [[0.1, 0.6],\n",
    "               [0.2, 0.1],\n",
    "               [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [tf.Variable(hidden_layer_weights),\n",
    "           tf.Variable(out_weights)]\n",
    "biases = [tf.Variable(tf.zeros(3)),\n",
    "          tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], \n",
    "                        [-1.0, -2.0, -3.0, -4.0], \n",
    "                        [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

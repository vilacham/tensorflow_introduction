{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neurais profundas\n",
    "\n",
    "Nos notebooks anteriores (`introducao.ipynb` e `not_mnist_with_tf.ipynb`) você aprendeu como construir uma rede neural com uma camada. Neste notebook você vai aprender a construir uma rede neural de várias camadas com o TensorFlow. Adicionar uma camada oculta a uma rede permite que ela modele funções mais complexas. Além disso, usar uma função de ativação não linear na camada oculta permite que ela modele funções não lineares.\n",
    "\n",
    "Este notebook foi construído a partir dos conhecimentos saprensentados na aula `Introdução às redes neurais`. Conceitos como feedforward e dropout são alguns dos muitos cruciais para o entendimento completo deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ReLu\n",
    "\n",
    "A primeira coisa que nós vamos aprender a implementar no TensorFlow é a camada oculta com função de ativação ReLu (Rectified Linear Unit, ou unidade retificada linear). A ReLu é uma função não linear que retorna 0 para entradas negativas e $x$ para todas as entradas $x > 0$.\n",
    "\n",
    "O TensorFlow fornece a função ReLu como [**`tf.nn.relu()`**](https://www.tensorflow.org/api_docs/python/tf/nn/relu), como visto no trecho de código abaixo:\n",
    "\n",
    "```\n",
    "# Camada oculta com função de ativação ReLu\n",
    "hidden_layer = tf.add(tf.matmul(X, hidden_w), hidden_b)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, output_w), output_b)\n",
    "```\n",
    "\n",
    "O trecho acima aplica a função [**`tf.nn.relu()`**](https://www.tensorflow.org/api_docs/python/tf/nn/relu) na camada `hidden_layer`, o que ba prática desativa quaisquer pesos negativos e funciona como chave liga/desliga. Camadas adicionais, como a camada `output`, aplicadas após a ativação transformam o modelo em uma função não-linear. Essa não-linearidade permite que a rede resolva problemas mais complexos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Implementação\n",
    "\n",
    "<img src='img/two_layer_network.png' width=600px>\n",
    "\n",
    "\n",
    "Na célula abaixo, você vai usar a função ReLu para transformar uma rede linear de uma camada em uma rede não-linear multicamadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.11      8.440001]\n",
      " [ 0.        0.      ]\n",
      " [24.010002 38.239998]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "output = None\n",
    "hidden_layer_weights = [[0.1, 0.2, 0.4],\n",
    "                        [0.4, 0.6, 0.6],\n",
    "                        [0.5, 0.9, 0.1],\n",
    "                        [0.8, 0.2, 0.8]]\n",
    "out_weights = [[0.1, 0.6],\n",
    "               [0.2, 0.1],\n",
    "               [0.7, 0.9]]\n",
    "\n",
    "# Weights and biases\n",
    "weights = [tf.Variable(hidden_layer_weights),\n",
    "           tf.Variable(out_weights)]\n",
    "biases = [tf.Variable(tf.zeros(3)),\n",
    "          tf.Variable(tf.zeros(2))]\n",
    "\n",
    "# Input\n",
    "features = tf.Variable([[1.0, 2.0, 3.0, 4.0], \n",
    "                        [-1.0, -2.0, -3.0, -4.0], \n",
    "                        [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "# TODO: Create Model\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "output = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])\n",
    "\n",
    "# TODO: Print session results\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Redes neurais profundas no TensorFlow\n",
    "\n",
    "Você viu anteriormente como criar um classificador logístico usando TensorFlow. Agora você vai ver como usar o classificador logístico para construir uma rede neural profunda.\n",
    "\n",
    "No passo a passo a seguir, vamos analisar um código escrito usando TensorFlow para classificar letras no conjunto de dados MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo o conjunto de dados\n",
    "\n",
    "Você vai usar o conjunto de dados MNIST fornecido pelo TensorFlow, que agrupa em lotes e codifica os dados usando a codificação one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('.', one_hot=True, reshape=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo hiperparâmetros\n",
    "\n",
    "O foco aqui é dado à arquitetura da rede neural multicamadas, não ao ajuste de hiperparâmetros, então dessa vez vamos simplesmente fornecer os hiperparâmetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 20\n",
    "batch_size = 128\n",
    "display_step = 1\n",
    "\n",
    "n_input = 28 * 28    # Dados de entrada do MNIST (imagens 28x28)\n",
    "n_classes = 10       # Total de classes do MNIST (algarismos de 0 a 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparâmetros da camada oculta\n",
    "\n",
    "A variável `n_hidden_layer` determina o tamanho da camada oculta na rede neural. Esse valor também é conhecido como a largura de uma camada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_layer = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesos e vieses\n",
    "\n",
    "Redes neurais profundas usam múltiplas camadas, sendo que cada camada exige seus próprios pesos e vieses. No código abaixo, os pesos e vieses da camada `hidden_layer` pertencem à camada oculta, e os pesos e vieses da camada `output` pertencem à camda de saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "           'output': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))}\n",
    "biases = {'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "          'output': tf.Variable(tf.random_normal([n_classes]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a rede neural fosse mais profunda, haveria pesos e vieses para cada camada adicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Os dados do conjunto MNIST são formados por imagens 28px por 28px em um único [canal](https://en.wikipedia.org/wiki/Channel_(digital_image%29). Na célula abaixo, a função [**`tf.reshape`**](https://www.tensorflow.org/api_docs/python/tf/reshape) remodela as matrizes 28px por 28px presentes em `x`, transformando-as em vetores 784px por 1px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percepton multicamadas\n",
    "\n",
    "<img src='img/mlp.png' width=500px>\n",
    "\n",
    "Você viu a função linear `Xw + b` antes. A combinação linear de funções usando uma ReLu gera uma rede de duas camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer with ReLu activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "# Output layer with linear activation\n",
    "logits = tf.add(tf.matmul(layer_1, weights['output']), biases['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimização\n",
    "\n",
    "A técnica utilizada será a mesma utilizada no notebook `introducao.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessão\n",
    "\n",
    "A biblioteca MNIST no TensorFlow fornece a possibilidade de receber o conjunto de dados em lotes. A função `mnist.train.next_batch()` retorna um subconjunto dos dados de treinamento, como visto a seguir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        total_batch = int(mnist.train.num_examples / batch_size)\n",
    "        for _ in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Redes neurais mais profundas\n",
    "\n",
    "<img src='img/layers.png' width=500px>\n",
    "\n",
    "Adicionar mais camadas à rede permite que você resolva problemas mais complicados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
